{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This notebook is to optimize hyperparameters for LSTM Model Classification using Randomized Search'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This notebook is to optimize hyperparameters for LSTM Model Classification using Randomized Search\"\"\"\n",
    "\n",
    "# seperated by npast_timesteps to make preprocessing easier (tried doing it inside the wrapper and ran into many shape issues) \n",
    "# kernal crashed when I tried to do it all in one for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 13:39:48.662462: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 13:39:48.663123: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-12 13:39:48.665464: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-12 13:39:48.671673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734039588.682591   11611 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734039588.685596   11611 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 13:39:48.697292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nrsim_lstm import NRSIM_LSTM as lstm\n",
    "from Archive import utils277b as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Set Parameters for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>pH</th>\n",
       "      <th>Hydrogen</th>\n",
       "      <th>Total Gas</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Radioactivity</th>\n",
       "      <th>Power</th>\n",
       "      <th>Reactor Safety</th>\n",
       "      <th>Injection of Air</th>\n",
       "      <th>Injection of Air Degree</th>\n",
       "      <th>Resin Overheat</th>\n",
       "      <th>Resin Overheat Degree</th>\n",
       "      <th>Fuel Element Failure</th>\n",
       "      <th>Fuel Element Failure Degree</th>\n",
       "      <th>Chemical Addition</th>\n",
       "      <th>Vent Gas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11.000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.998</td>\n",
       "      <td>50.112998</td>\n",
       "      <td>60.112998</td>\n",
       "      <td>500.732703</td>\n",
       "      <td>2104.971916</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.996</td>\n",
       "      <td>50.225941</td>\n",
       "      <td>60.225941</td>\n",
       "      <td>501.463398</td>\n",
       "      <td>2109.930204</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.994</td>\n",
       "      <td>50.338517</td>\n",
       "      <td>60.338517</td>\n",
       "      <td>502.190083</td>\n",
       "      <td>2114.861274</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.992</td>\n",
       "      <td>50.450414</td>\n",
       "      <td>60.450414</td>\n",
       "      <td>502.910764</td>\n",
       "      <td>2119.751611</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time      pH   Hydrogen  Total Gas  Temperature     Pressure  \\\n",
       "0     0  11.000  50.000000  60.000000   500.000000  2100.000000   \n",
       "1     1  10.998  50.112998  60.112998   500.732703  2104.971916   \n",
       "2     2  10.996  50.225941  60.225941   501.463398  2109.930204   \n",
       "3     3  10.994  50.338517  60.338517   502.190083  2114.861274   \n",
       "4     4  10.992  50.450414  60.450414   502.910764  2119.751611   \n",
       "\n",
       "   Radioactivity  Power  Reactor Safety Injection of Air  \\\n",
       "0           10.0  100.0               0              NaN   \n",
       "1           10.0  100.0               0              NaN   \n",
       "2           10.0  100.0               0              NaN   \n",
       "3           10.0  100.0               0              NaN   \n",
       "4           10.0  100.0               0              NaN   \n",
       "\n",
       "  Injection of Air Degree Resin Overheat Resin Overheat Degree  \\\n",
       "0                     NaN            NaN                   NaN   \n",
       "1                     NaN            NaN                   NaN   \n",
       "2                     NaN            NaN                   NaN   \n",
       "3                     NaN            NaN                   NaN   \n",
       "4                     NaN            NaN                   NaN   \n",
       "\n",
       "  Fuel Element Failure Fuel Element Failure Degree  Chemical Addition  \\\n",
       "0                  NaN                         NaN              False   \n",
       "1                  NaN                         NaN              False   \n",
       "2                  NaN                         NaN              False   \n",
       "3                  NaN                         NaN              False   \n",
       "4                  NaN                         NaN              False   \n",
       "\n",
       "   Vent Gas  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in data\n",
    "data = pd.read_csv(\"data/Sim_all_casualties.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11611/1598807462.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "numerical = ['Time', 'pH', 'Hydrogen', 'Total Gas', 'Temperature', 'Pressure','Radioactivity', 'Power']\n",
    "categorical = ['Reactor Safety', 'Injection of Air', 'Injection of Air Degree', \n",
    "               'Resin Overheat', 'Resin Overheat Degree', 'Fuel Element Failure', \n",
    "               'Fuel Element Failure Degree', 'Chemical Addition', 'Vent Gas']\n",
    "\n",
    "\n",
    "# In case there are NaN values\n",
    "data = data.fillna(False)\n",
    "data[categorical] = data[categorical].astype(int) # set categorical as integer\n",
    "\n",
    "# make sure no NaN values\n",
    "unique_values = data.apply(pd.Series.unique)\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training matrices\n",
    "X = np.array(data[numerical].drop(columns=['Time']))\n",
    "Y = np.array(data[categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "class PreprocessingWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_timesteps=1, n_features=7, neurons=[32, 32], activation='tanh', \n",
    "                 optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                 dropout=0.2, conv_layer=False, nfilters=64, conv_act='relu', pool_size=2, \n",
    "                 classification=True, n_predicted_timesteps = 1, epochs = 50, batch = 64, n_predicted_features = 9):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.dropout = dropout\n",
    "        self.conv_layer = conv_layer\n",
    "        self.nfilters = nfilters\n",
    "        self.conv_act = conv_act\n",
    "        self.pool_size = pool_size\n",
    "        self.classification = classification\n",
    "        self.n_predicted_timesteps = n_predicted_timesteps\n",
    "        self.epochs = epochs\n",
    "        self.batch  = batch\n",
    "        self.n_predicted_features = n_predicted_features\n",
    "        \n",
    "    def create_model(self):\n",
    "\n",
    "        model = lstm(neurons=self.neurons, \n",
    "                     activation_func = self.activation, \n",
    "                     nTimesteps = self.n_timesteps, \n",
    "                     nFeatures = self.n_features, \n",
    "                     npredTimesteps = 1, \n",
    "                     npredFeatures = self.n_predicted_features, \n",
    "                     model_optimizer = self.optimizer, \n",
    "                     model_loss = self.loss, \n",
    "                     model_metrics = self.metrics, \n",
    "                     dropout=self.dropout, \n",
    "                     conv_layer=self.conv_layer, \n",
    "                     nfilters=self.nfilters, \n",
    "                     cact =self.conv_act, \n",
    "                     cpool=2,\n",
    "                     classify=self.classification)\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.classification:\n",
    "            self.classes_ = np.unique(y)\n",
    "        # print(f'shape of Y: {Y.shape}')\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X, y, nEpochs = 1, nBatches= 64, val_split = 0.2, verb = 2, shuf = False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        # print(f'shape of x (predict): {X.shape}')\n",
    "        y_pred = self.model.predict(X)\n",
    "        # print(f'shape of y_pred: {y_pred.shape}')\n",
    "        y_pred = (y_pred >= 0.5).astype(float)\n",
    "        # print(y_pred[0][0])\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(y_true, y_pred):\n",
    "    # print(f\"Shape of y_true in custom scoring function: {y_true.shape}\")\n",
    "    # print(y_true[0][0])\n",
    "    # print(f\"Shape of y_pred in custom scoring function: {y_pred.shape}\")\n",
    "    # print(y_pred[0][0])\n",
    "\n",
    "    # # flatten\n",
    "    y_true_2d = y_true.reshape(-1, y_true.shape[2])  # Shape becomes (2148, 9)\n",
    "    y_pred_2d = y_pred.reshape(-1, y_pred.shape[2])  # Shape becomes (2148, 9)\n",
    "\n",
    "    y_true_2d = y_true_2d.astype(int)\n",
    "    y_pred_2d = y_pred_2d.astype(int)\n",
    "\n",
    "    # print(f'shape of true and pred after reshape: {y_true_2d.shape} and {y_pred_2d.shape}')\n",
    "    # print(f'type: {type(y_true_2d[0][0])}')\n",
    "    # print(f\"Data type of y_true: {y_true_2d.dtype}\")\n",
    "    # print(f\"Data type of y_pred: {y_pred_2d.dtype}\")\n",
    "\n",
    "\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    accuracy = correct / y_true.size\n",
    "\n",
    "\n",
    "    # Compute accuracy or any custom metric\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier  # Example model\n",
    "from scipy.stats import randint, uniform  # For defining distributions\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 npast_timesteps\n",
    "- Timesteps tried are defined outside of the RandomizedSearch to make preprocessing easier and avoid nan scoring values\n",
    "- Tried to run 25, 50, 75 timesteps using for loop and list, but kernal died or randomized search terminated due to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 13:39:50.391238: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "9/9 - 3s - 302ms/step - accuracy: 0.0017 - loss: 0.6433 - val_accuracy: 0.0000e+00 - val_loss: 0.5828\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 3s - 143ms/step - accuracy: 0.1039 - loss: 0.5740 - val_accuracy: 0.0592 - val_loss: 0.5010\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.2s\n",
      "27/27 - 3s - 122ms/step - accuracy: 0.2031 - loss: 0.5226 - val_accuracy: 0.9698 - val_loss: 0.3824\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "36/36 - 4s - 113ms/step - accuracy: 0.0589 - loss: 0.4622 - val_accuracy: 0.0000e+00 - val_loss: 0.3170\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "45/45 - 4s - 86ms/step - accuracy: 0.3862 - loss: 0.4150 - val_accuracy: 0.9874 - val_loss: 0.3057\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "9/9 - 3s - 381ms/step - accuracy: 0.1364 - loss: 0.6779 - val_accuracy: 0.4514 - val_loss: 0.6558\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "18/18 - 4s - 201ms/step - accuracy: 0.0734 - loss: 0.6640 - val_accuracy: 0.0000e+00 - val_loss: 0.6327\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 3s - 117ms/step - accuracy: 0.0891 - loss: 0.5856 - val_accuracy: 0.5721 - val_loss: 0.3717\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "36/36 - 3s - 93ms/step - accuracy: 0.1021 - loss: 0.5857 - val_accuracy: 0.9284 - val_loss: 0.3417\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "45/45 - 4s - 93ms/step - accuracy: 0.4794 - loss: 0.5816 - val_accuracy: 0.9874 - val_loss: 0.3890\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "9/9 - 4s - 437ms/step - accuracy: 0.4965 - loss: 0.6420 - val_accuracy: 0.6806 - val_loss: 0.5008\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "18/18 - 5s - 267ms/step - accuracy: 8.7336e-04 - loss: 0.6712 - val_accuracy: 0.0000e+00 - val_loss: 0.4714\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 5s - 201ms/step - accuracy: 0.0518 - loss: 0.6465 - val_accuracy: 0.9698 - val_loss: 0.5339\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.6s\n",
      "36/36 - 6s - 156ms/step - accuracy: 0.1694 - loss: 0.6149 - val_accuracy: 0.0000e+00 - val_loss: 0.4328\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "45/45 - 6s - 142ms/step - accuracy: 0.3959 - loss: 0.5106 - val_accuracy: 0.0000e+00 - val_loss: 0.4066\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.5s\n",
      "9/9 - 3s - 282ms/step - accuracy: 0.0245 - loss: 0.6761 - val_accuracy: 0.0000e+00 - val_loss: 0.6402\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.3s\n",
      "18/18 - 4s - 200ms/step - accuracy: 8.7336e-04 - loss: 0.5654 - val_accuracy: 0.0000e+00 - val_loss: 0.4981\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 3s - 116ms/step - accuracy: 0.8766 - loss: 0.5708 - val_accuracy: 0.9698 - val_loss: 0.4170\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "36/36 - 3s - 92ms/step - accuracy: 0.5037 - loss: 0.5445 - val_accuracy: 0.9424 - val_loss: 0.3662\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "45/45 - 4s - 80ms/step - accuracy: 0.1386 - loss: 0.4807 - val_accuracy: 0.9008 - val_loss: 0.3437\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "9/9 - 3s - 279ms/step - accuracy: 0.0000e+00 - loss: 0.6842 - val_accuracy: 0.0000e+00 - val_loss: 0.6539\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.2s\n",
      "18/18 - 3s - 161ms/step - accuracy: 0.0114 - loss: 0.6752 - val_accuracy: 0.0000e+00 - val_loss: 0.6374\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 3s - 118ms/step - accuracy: 0.0215 - loss: 0.6351 - val_accuracy: 0.0209 - val_loss: 0.5417\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "36/36 - 4s - 99ms/step - accuracy: 8.7298e-04 - loss: 0.6163 - val_accuracy: 0.0000e+00 - val_loss: 0.4168\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "45/45 - 4s - 86ms/step - accuracy: 0.1568 - loss: 0.5877 - val_accuracy: 0.9874 - val_loss: 0.4116\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "9/9 - 3s - 296ms/step - accuracy: 0.0000e+00 - loss: 0.6560 - val_accuracy: 0.0000e+00 - val_loss: 0.5963\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 166ms/step - accuracy: 0.0026 - loss: 0.5659 - val_accuracy: 0.0000e+00 - val_loss: 0.4902\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 123ms/step - accuracy: 0.7136 - loss: 0.4931 - val_accuracy: 0.9698 - val_loss: 0.3630\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "36/36 - 5s - 129ms/step - accuracy: 0.8472 - loss: 0.4239 - val_accuracy: 0.9424 - val_loss: 0.3193\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "45/45 - 4s - 94ms/step - accuracy: 0.6487 - loss: 0.4238 - val_accuracy: 0.9874 - val_loss: 0.2608\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "9/9 - 4s - 441ms/step - accuracy: 0.2727 - loss: 0.6172 - val_accuracy: 1.0000 - val_loss: 0.4883\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "18/18 - 5s - 263ms/step - accuracy: 0.0157 - loss: 0.5055 - val_accuracy: 0.0000e+00 - val_loss: 0.4714\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "27/27 - 5s - 200ms/step - accuracy: 0.7014 - loss: 0.4358 - val_accuracy: 0.9698 - val_loss: 0.3364\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.6s\n",
      "36/36 - 6s - 172ms/step - accuracy: 0.7145 - loss: 0.4097 - val_accuracy: 0.9424 - val_loss: 0.3023\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.5s\n",
      "45/45 - 8s - 176ms/step - accuracy: 0.5185 - loss: 0.4553 - val_accuracy: 0.9874 - val_loss: 0.3232\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.2s\n",
      "9/9 - 4s - 447ms/step - accuracy: 0.0332 - loss: 0.6683 - val_accuracy: 0.0000e+00 - val_loss: 0.5921\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "18/18 - 5s - 258ms/step - accuracy: 0.6410 - loss: 0.6536 - val_accuracy: 0.9303 - val_loss: 0.5280\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 6s - 215ms/step - accuracy: 0.2398 - loss: 0.5416 - val_accuracy: 0.0000e+00 - val_loss: 0.3631\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "36/36 - 6s - 168ms/step - accuracy: 0.0759 - loss: 0.5981 - val_accuracy: 0.0000e+00 - val_loss: 0.3902\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "45/45 - 7s - 149ms/step - accuracy: 0.0024 - loss: 0.5461 - val_accuracy: 0.0000e+00 - val_loss: 0.4351\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.9s\n",
      "9/9 - 5s - 585ms/step - accuracy: 0.2815 - loss: 0.5667 - val_accuracy: 0.0000e+00 - val_loss: 0.4268\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "18/18 - 5s - 268ms/step - accuracy: 0.5825 - loss: 0.4393 - val_accuracy: 0.9303 - val_loss: 0.4502\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "27/27 - 5s - 197ms/step - accuracy: 0.5221 - loss: 0.4320 - val_accuracy: 0.9698 - val_loss: 0.3129\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "36/36 - 6s - 165ms/step - accuracy: 0.4387 - loss: 0.4532 - val_accuracy: 0.9424 - val_loss: 0.3139\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "45/45 - 7s - 149ms/step - accuracy: 0.2898 - loss: 0.4026 - val_accuracy: 0.9874 - val_loss: 0.3026\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.9s\n",
      "9/9 - 3s - 306ms/step - accuracy: 0.0175 - loss: 0.6735 - val_accuracy: 0.0000e+00 - val_loss: 0.6385\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "18/18 - 3s - 169ms/step - accuracy: 0.0227 - loss: 0.6647 - val_accuracy: 0.0000e+00 - val_loss: 0.6238\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 124ms/step - accuracy: 0.0466 - loss: 0.6026 - val_accuracy: 0.0000e+00 - val_loss: 0.4403\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "36/36 - 4s - 104ms/step - accuracy: 0.0135 - loss: 0.6188 - val_accuracy: 0.0000e+00 - val_loss: 0.5141\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "45/45 - 6s - 125ms/step - accuracy: 0.6428 - loss: 0.5145 - val_accuracy: 0.9078 - val_loss: 0.3525\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "9/9 - 3s - 299ms/step - accuracy: 0.0000e+00 - loss: 0.6969 - val_accuracy: 0.0000e+00 - val_loss: 0.6858\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 3s - 164ms/step - accuracy: 0.0000e+00 - loss: 0.6700 - val_accuracy: 0.0000e+00 - val_loss: 0.6493\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 123ms/step - accuracy: 0.0116 - loss: 0.6512 - val_accuracy: 0.0000e+00 - val_loss: 0.5361\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "36/36 - 4s - 101ms/step - accuracy: 0.0026 - loss: 0.5235 - val_accuracy: 0.0000e+00 - val_loss: 0.3735\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "45/45 - 4s - 85ms/step - accuracy: 0.0192 - loss: 0.5798 - val_accuracy: 0.0000e+00 - val_loss: 0.3974\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "9/9 - 3s - 292ms/step - accuracy: 0.0017 - loss: 0.5941 - val_accuracy: 0.0000e+00 - val_loss: 0.4804\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 167ms/step - accuracy: 0.1362 - loss: 0.5686 - val_accuracy: 0.2718 - val_loss: 0.4937\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 4s - 131ms/step - accuracy: 0.0536 - loss: 0.5024 - val_accuracy: 0.0000e+00 - val_loss: 0.3512\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "36/36 - 4s - 110ms/step - accuracy: 0.4470 - loss: 0.4859 - val_accuracy: 0.9424 - val_loss: 0.3059\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "45/45 - 4s - 91ms/step - accuracy: 0.1264 - loss: 0.4440 - val_accuracy: 0.0000e+00 - val_loss: 0.3459\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "9/9 - 3s - 301ms/step - accuracy: 0.0245 - loss: 0.6317 - val_accuracy: 0.0000e+00 - val_loss: 0.5508\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 5s - 267ms/step - accuracy: 0.4349 - loss: 0.5533 - val_accuracy: 0.9303 - val_loss: 0.4905\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "27/27 - 4s - 140ms/step - accuracy: 0.3778 - loss: 0.4841 - val_accuracy: 0.9698 - val_loss: 0.3623\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "36/36 - 4s - 108ms/step - accuracy: 0.4828 - loss: 0.4516 - val_accuracy: 0.9424 - val_loss: 0.2975\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "45/45 - 4s - 94ms/step - accuracy: 0.4267 - loss: 0.4754 - val_accuracy: 0.9874 - val_loss: 0.3117\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "9/9 - 3s - 297ms/step - accuracy: 0.0559 - loss: 0.6747 - val_accuracy: 0.0000e+00 - val_loss: 0.6457\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 166ms/step - accuracy: 0.1380 - loss: 0.6320 - val_accuracy: 0.0000e+00 - val_loss: 0.5725\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 126ms/step - accuracy: 0.2759 - loss: 0.5774 - val_accuracy: 0.9698 - val_loss: 0.4082\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "36/36 - 4s - 108ms/step - accuracy: 0.4291 - loss: 0.5746 - val_accuracy: 0.9424 - val_loss: 0.4432\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "45/45 - 4s - 89ms/step - accuracy: 0.5915 - loss: 0.5362 - val_accuracy: 0.9874 - val_loss: 0.3464\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "9/9 - 3s - 289ms/step - accuracy: 0.0122 - loss: 0.6392 - val_accuracy: 0.0000e+00 - val_loss: 0.5648\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 168ms/step - accuracy: 0.3249 - loss: 0.5203 - val_accuracy: 0.9303 - val_loss: 0.4417\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "27/27 - 3s - 126ms/step - accuracy: 0.1315 - loss: 0.5214 - val_accuracy: 0.9698 - val_loss: 0.3981\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "36/36 - 5s - 127ms/step - accuracy: 0.2876 - loss: 0.4900 - val_accuracy: 0.9424 - val_loss: 0.3291\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "45/45 - 4s - 93ms/step - accuracy: 0.0538 - loss: 0.4471 - val_accuracy: 0.0000e+00 - val_loss: 0.3423\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "9/9 - 5s - 543ms/step - accuracy: 0.0367 - loss: 0.6605 - val_accuracy: 0.0000e+00 - val_loss: 0.6358\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "18/18 - 3s - 169ms/step - accuracy: 0.2341 - loss: 0.6591 - val_accuracy: 0.2892 - val_loss: 0.6331\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 125ms/step - accuracy: 0.3976 - loss: 0.6690 - val_accuracy: 0.0000e+00 - val_loss: 0.6120\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "36/36 - 4s - 101ms/step - accuracy: 0.0026 - loss: 0.6502 - val_accuracy: 0.0000e+00 - val_loss: 0.4938\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "45/45 - 4s - 99ms/step - accuracy: 0.4860 - loss: 0.4764 - val_accuracy: 0.0000e+00 - val_loss: 0.4152\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "9/9 - 4s - 459ms/step - accuracy: 0.0000e+00 - loss: 0.5646 - val_accuracy: 0.0000e+00 - val_loss: 0.4312\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "18/18 - 5s - 275ms/step - accuracy: 0.4148 - loss: 0.4910 - val_accuracy: 0.9303 - val_loss: 0.4480\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "27/27 - 5s - 199ms/step - accuracy: 0.3015 - loss: 0.4911 - val_accuracy: 0.9698 - val_loss: 0.3691\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "36/36 - 6s - 170ms/step - accuracy: 0.2968 - loss: 0.4082 - val_accuracy: 0.9424 - val_loss: 0.2869\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "45/45 - 7s - 150ms/step - accuracy: 0.3694 - loss: 0.4156 - val_accuracy: 0.9874 - val_loss: 0.3187\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.0s\n",
      "9/9 - 3s - 298ms/step - accuracy: 0.0402 - loss: 0.6849 - val_accuracy: 0.0000e+00 - val_loss: 0.6688\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 5s - 285ms/step - accuracy: 0.0262 - loss: 0.6224 - val_accuracy: 0.2474 - val_loss: 0.5148\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "27/27 - 5s - 167ms/step - accuracy: 0.3888 - loss: 0.5942 - val_accuracy: 0.9698 - val_loss: 0.4088\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "36/36 - 4s - 116ms/step - accuracy: 0.0607 - loss: 0.6371 - val_accuracy: 0.9424 - val_loss: 0.4265\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "45/45 - 5s - 105ms/step - accuracy: 0.5405 - loss: 0.4914 - val_accuracy: 0.9874 - val_loss: 0.2754\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "9/9 - 3s - 340ms/step - accuracy: 0.0000e+00 - loss: 0.6824 - val_accuracy: 0.0000e+00 - val_loss: 0.6474\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "18/18 - 4s - 227ms/step - accuracy: 0.0096 - loss: 0.6244 - val_accuracy: 0.0000e+00 - val_loss: 0.5090\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 4s - 163ms/step - accuracy: 0.3597 - loss: 0.6122 - val_accuracy: 0.9698 - val_loss: 0.4217\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "36/36 - 5s - 133ms/step - accuracy: 0.2370 - loss: 0.5733 - val_accuracy: 0.9424 - val_loss: 0.3247\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "45/45 - 5s - 113ms/step - accuracy: 0.6638 - loss: 0.5180 - val_accuracy: 0.2919 - val_loss: 0.3906\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "9/9 - 3s - 356ms/step - accuracy: 0.0017 - loss: 0.6354 - val_accuracy: 0.0000e+00 - val_loss: 0.5440\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "18/18 - 4s - 203ms/step - accuracy: 0.7057 - loss: 0.5390 - val_accuracy: 0.9303 - val_loss: 0.5022\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "27/27 - 4s - 147ms/step - accuracy: 0.2532 - loss: 0.5313 - val_accuracy: 0.9698 - val_loss: 0.3978\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "36/36 - 5s - 132ms/step - accuracy: 0.8869 - loss: 0.4406 - val_accuracy: 0.9424 - val_loss: 0.3122\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "45/45 - 5s - 116ms/step - accuracy: 0.8642 - loss: 0.4421 - val_accuracy: 0.9874 - val_loss: 0.2814\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 5s - 99ms/step - accuracy: 0.2905 - loss: 0.4025 - val_accuracy: 0.9674 - val_loss: 0.2173\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "9/9 - 3s - 378ms/step - accuracy: 0.0000e+00 - loss: 0.6670 - val_accuracy: 0.0000e+00 - val_loss: 0.6143\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "18/18 - 4s - 199ms/step - accuracy: 0.0017 - loss: 0.6097 - val_accuracy: 0.0000e+00 - val_loss: 0.5357\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 4s - 134ms/step - accuracy: 0.8830 - loss: 0.5234 - val_accuracy: 0.9698 - val_loss: 0.3737\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "36/36 - 4s - 100ms/step - accuracy: 0.7525 - loss: 0.5514 - val_accuracy: 0.9424 - val_loss: 0.4145\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "45/45 - 4s - 80ms/step - accuracy: 0.6606 - loss: 0.4702 - val_accuracy: 0.9874 - val_loss: 0.3212\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "9/9 - 3s - 302ms/step - accuracy: 0.0140 - loss: 0.6858 - val_accuracy: 0.0000e+00 - val_loss: 0.6733\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 157ms/step - accuracy: 0.0559 - loss: 0.6796 - val_accuracy: 0.0000e+00 - val_loss: 0.6617\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 6s - 226ms/step - accuracy: 0.0146 - loss: 0.6166 - val_accuracy: 0.0000e+00 - val_loss: 0.4942\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.9s\n",
      "36/36 - 3s - 93ms/step - accuracy: 0.6203 - loss: 0.5805 - val_accuracy: 0.9424 - val_loss: 0.3830\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "45/45 - 3s - 72ms/step - accuracy: 0.6037 - loss: 0.5700 - val_accuracy: 0.9874 - val_loss: 0.2654\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "9/9 - 4s - 431ms/step - accuracy: 0.0385 - loss: 0.6890 - val_accuracy: 0.0000e+00 - val_loss: 0.6808\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "18/18 - 4s - 221ms/step - accuracy: 0.6838 - loss: 0.6724 - val_accuracy: 0.9303 - val_loss: 0.6219\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 4s - 147ms/step - accuracy: 0.2427 - loss: 0.6219 - val_accuracy: 0.9698 - val_loss: 0.3720\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "36/36 - 4s - 112ms/step - accuracy: 0.1584 - loss: 0.6189 - val_accuracy: 0.9424 - val_loss: 0.3488\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "45/45 - 4s - 91ms/step - accuracy: 0.1575 - loss: 0.5665 - val_accuracy: 0.0000e+00 - val_loss: 0.4527\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "9/9 - 3s - 305ms/step - accuracy: 0.0000e+00 - loss: 0.6800 - val_accuracy: 0.0000e+00 - val_loss: 0.6608\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 3s - 177ms/step - accuracy: 0.1144 - loss: 0.6585 - val_accuracy: 0.0000e+00 - val_loss: 0.6227\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "27/27 - 3s - 112ms/step - accuracy: 0.0017 - loss: 0.6410 - val_accuracy: 0.0000e+00 - val_loss: 0.5547\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "36/36 - 3s - 85ms/step - accuracy: 0.1763 - loss: 0.5976 - val_accuracy: 0.9424 - val_loss: 0.4360\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "45/45 - 3s - 75ms/step - accuracy: 0.0517 - loss: 0.5823 - val_accuracy: 0.0000e+00 - val_loss: 0.4556\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "9/9 - 3s - 378ms/step - accuracy: 0.0000e+00 - loss: 0.6847 - val_accuracy: 0.0000e+00 - val_loss: 0.6670\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "18/18 - 3s - 189ms/step - accuracy: 0.0026 - loss: 0.6647 - val_accuracy: 0.0000e+00 - val_loss: 0.6181\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 3s - 109ms/step - accuracy: 0.0012 - loss: 0.6067 - val_accuracy: 0.0000e+00 - val_loss: 0.4189\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "36/36 - 3s - 83ms/step - accuracy: 0.5303 - loss: 0.6209 - val_accuracy: 0.9424 - val_loss: 0.4438\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "45/45 - 3s - 74ms/step - accuracy: 0.8359 - loss: 0.5045 - val_accuracy: 0.9874 - val_loss: 0.3048\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "9/9 - 7s - 736ms/step - accuracy: 0.0000e+00 - loss: 0.6747 - val_accuracy: 0.0000e+00 - val_loss: 0.6386\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.4s\n",
      "18/18 - 3s - 179ms/step - accuracy: 0.2131 - loss: 0.6141 - val_accuracy: 0.9303 - val_loss: 0.5533\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "27/27 - 3s - 115ms/step - accuracy: 0.5995 - loss: 0.5781 - val_accuracy: 0.9698 - val_loss: 0.4330\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "36/36 - 3s - 90ms/step - accuracy: 0.3173 - loss: 0.5471 - val_accuracy: 0.9424 - val_loss: 0.3793\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "45/45 - 3s - 73ms/step - accuracy: 0.4518 - loss: 0.4963 - val_accuracy: 0.9874 - val_loss: 0.3263\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "9/9 - 4s - 420ms/step - accuracy: 0.3584 - loss: 0.6833 - val_accuracy: 1.0000 - val_loss: 0.6632\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "18/18 - 4s - 212ms/step - accuracy: 0.6079 - loss: 0.6312 - val_accuracy: 0.9303 - val_loss: 0.5648\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "27/27 - 4s - 142ms/step - accuracy: 0.8428 - loss: 0.6038 - val_accuracy: 0.9698 - val_loss: 0.4501\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "36/36 - 4s - 118ms/step - accuracy: 0.0144 - loss: 0.5692 - val_accuracy: 0.0000e+00 - val_loss: 0.3788\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "45/45 - 4s - 94ms/step - accuracy: 0.1323 - loss: 0.5202 - val_accuracy: 0.0000e+00 - val_loss: 0.3408\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "9/9 - 4s - 413ms/step - accuracy: 0.0367 - loss: 0.6903 - val_accuracy: 0.0000e+00 - val_loss: 0.6860\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "18/18 - 4s - 226ms/step - accuracy: 0.4192 - loss: 0.6871 - val_accuracy: 0.9303 - val_loss: 0.6790\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 4s - 145ms/step - accuracy: 0.0856 - loss: 0.6568 - val_accuracy: 0.0000e+00 - val_loss: 0.4863\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "36/36 - 4s - 114ms/step - accuracy: 0.5120 - loss: 0.6345 - val_accuracy: 0.9424 - val_loss: 0.3290\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "45/45 - 4s - 99ms/step - accuracy: 0.3453 - loss: 0.5926 - val_accuracy: 0.6732 - val_loss: 0.3686\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "9/9 - 4s - 431ms/step - accuracy: 0.0052 - loss: 0.6864 - val_accuracy: 0.0000e+00 - val_loss: 0.6730\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "18/18 - 4s - 240ms/step - accuracy: 0.7450 - loss: 0.6482 - val_accuracy: 0.9303 - val_loss: 0.5853\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 5s - 168ms/step - accuracy: 0.2119 - loss: 0.6185 - val_accuracy: 0.9698 - val_loss: 0.4797\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "36/36 - 10s - 270ms/step - accuracy: 0.0205 - loss: 0.5513 - val_accuracy: 0.0000e+00 - val_loss: 0.3613\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  11.0s\n",
      "45/45 - 7s - 149ms/step - accuracy: 0.2727 - loss: 0.5047 - val_accuracy: 0.0000e+00 - val_loss: 0.3224\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "9/9 - 4s - 421ms/step - accuracy: 0.1189 - loss: 0.6681 - val_accuracy: 0.0000e+00 - val_loss: 0.6102\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "18/18 - 4s - 210ms/step - accuracy: 0.0236 - loss: 0.6600 - val_accuracy: 0.0000e+00 - val_loss: 0.5937\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 4s - 131ms/step - accuracy: 0.0256 - loss: 0.5749 - val_accuracy: 0.0000e+00 - val_loss: 0.3751\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "36/36 - 4s - 103ms/step - accuracy: 0.5251 - loss: 0.5516 - val_accuracy: 0.9424 - val_loss: 0.3160\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "45/45 - 4s - 94ms/step - accuracy: 0.1983 - loss: 0.5023 - val_accuracy: 0.9874 - val_loss: 0.2608\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "9/9 - 3s - 319ms/step - accuracy: 0.6031 - loss: 0.6896 - val_accuracy: 1.0000 - val_loss: 0.6848\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "18/18 - 3s - 158ms/step - accuracy: 0.6044 - loss: 0.6818 - val_accuracy: 0.9303 - val_loss: 0.6689\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 107ms/step - accuracy: 0.1088 - loss: 0.6558 - val_accuracy: 0.0000e+00 - val_loss: 0.5515\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "36/36 - 3s - 81ms/step - accuracy: 0.0044 - loss: 0.6231 - val_accuracy: 0.0000e+00 - val_loss: 0.4173\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "45/45 - 3s - 66ms/step - accuracy: 0.0049 - loss: 0.6017 - val_accuracy: 0.0000e+00 - val_loss: 0.4801\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "9/9 - 3s - 305ms/step - accuracy: 0.0385 - loss: 0.6743 - val_accuracy: 0.0000e+00 - val_loss: 0.6406\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "18/18 - 3s - 162ms/step - accuracy: 0.0358 - loss: 0.6141 - val_accuracy: 0.0000e+00 - val_loss: 0.5520\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 108ms/step - accuracy: 0.0373 - loss: 0.5978 - val_accuracy: 0.0000e+00 - val_loss: 0.4551\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "36/36 - 3s - 85ms/step - accuracy: 0.1327 - loss: 0.5650 - val_accuracy: 0.9424 - val_loss: 0.3757\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "45/45 - 3s - 74ms/step - accuracy: 0.4172 - loss: 0.4806 - val_accuracy: 0.0000e+00 - val_loss: 0.3305\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "9/9 - 5s - 581ms/step - accuracy: 0.0385 - loss: 0.6739 - val_accuracy: 0.0000e+00 - val_loss: 0.6436\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "18/18 - 4s - 223ms/step - accuracy: 0.0000e+00 - loss: 0.6197 - val_accuracy: 0.0000e+00 - val_loss: 0.5741\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 3s - 121ms/step - accuracy: 0.2625 - loss: 0.5865 - val_accuracy: 0.9698 - val_loss: 0.4438\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "36/36 - 4s - 109ms/step - accuracy: 0.0310 - loss: 0.5827 - val_accuracy: 0.0000e+00 - val_loss: 0.4202\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "45/45 - 3s - 70ms/step - accuracy: 0.1526 - loss: 0.5163 - val_accuracy: 0.0000e+00 - val_loss: 0.3594\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "9/9 - 3s - 319ms/step - accuracy: 0.1329 - loss: 0.6808 - val_accuracy: 0.0000e+00 - val_loss: 0.6536\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "18/18 - 3s - 165ms/step - accuracy: 0.6655 - loss: 0.6336 - val_accuracy: 0.9303 - val_loss: 0.5122\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 114ms/step - accuracy: 0.0291 - loss: 0.6272 - val_accuracy: 0.0000e+00 - val_loss: 0.4136\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "36/36 - 3s - 86ms/step - accuracy: 0.1567 - loss: 0.5600 - val_accuracy: 0.9424 - val_loss: 0.2794\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "45/45 - 9s - 197ms/step - accuracy: 0.2011 - loss: 0.5181 - val_accuracy: 0.3785 - val_loss: 0.3198\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.7s\n",
      "9/9 - 3s - 340ms/step - accuracy: 0.5367 - loss: 0.6650 - val_accuracy: 1.0000 - val_loss: 0.6137\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "18/18 - 3s - 168ms/step - accuracy: 0.0148 - loss: 0.6315 - val_accuracy: 0.0000e+00 - val_loss: 0.5895\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 120ms/step - accuracy: 0.2544 - loss: 0.5858 - val_accuracy: 0.9698 - val_loss: 0.4495\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "36/36 - 4s - 100ms/step - accuracy: 0.2052 - loss: 0.5205 - val_accuracy: 0.0000e+00 - val_loss: 0.3496\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "45/45 - 3s - 70ms/step - accuracy: 0.1002 - loss: 0.5293 - val_accuracy: 0.0000e+00 - val_loss: 0.3714\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "9/9 - 3s - 318ms/step - accuracy: 0.0157 - loss: 0.6897 - val_accuracy: 0.0000e+00 - val_loss: 0.6839\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "18/18 - 3s - 161ms/step - accuracy: 8.7336e-04 - loss: 0.6739 - val_accuracy: 0.0000e+00 - val_loss: 0.6517\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 108ms/step - accuracy: 5.8207e-04 - loss: 0.6695 - val_accuracy: 0.0000e+00 - val_loss: 0.6055\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "36/36 - 3s - 79ms/step - accuracy: 0.0000e+00 - loss: 0.6328 - val_accuracy: 0.0000e+00 - val_loss: 0.4076\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "45/45 - 3s - 64ms/step - accuracy: 0.7699 - loss: 0.6456 - val_accuracy: 0.9874 - val_loss: 0.3503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "9/9 - 4s - 393ms/step - accuracy: 0.0927 - loss: 0.6843 - val_accuracy: 0.0000e+00 - val_loss: 0.6648\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "18/18 - 4s - 209ms/step - accuracy: 0.0026 - loss: 0.6540 - val_accuracy: 0.0000e+00 - val_loss: 0.5948\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 4s - 147ms/step - accuracy: 0.0204 - loss: 0.6107 - val_accuracy: 0.0000e+00 - val_loss: 0.4715\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "36/36 - 4s - 114ms/step - accuracy: 0.8904 - loss: 0.5796 - val_accuracy: 0.9424 - val_loss: 0.4024\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "45/45 - 4s - 93ms/step - accuracy: 0.1145 - loss: 0.5262 - val_accuracy: 0.0000e+00 - val_loss: 0.3553\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "9/9 - 3s - 370ms/step - accuracy: 0.1678 - loss: 0.6877 - val_accuracy: 0.0000e+00 - val_loss: 0.6792\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "18/18 - 3s - 150ms/step - accuracy: 0.6472 - loss: 0.6744 - val_accuracy: 0.9303 - val_loss: 0.6447\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "27/27 - 3s - 105ms/step - accuracy: 0.0233 - loss: 0.6519 - val_accuracy: 0.0302 - val_loss: 0.5255\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "36/36 - 3s - 84ms/step - accuracy: 0.1004 - loss: 0.5981 - val_accuracy: 0.0750 - val_loss: 0.3610\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "45/45 - 3s - 68ms/step - accuracy: 0.8736 - loss: 0.5148 - val_accuracy: 0.0000e+00 - val_loss: 0.3913\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "9/9 - 3s - 299ms/step - accuracy: 0.0192 - loss: 0.6812 - val_accuracy: 0.0000e+00 - val_loss: 0.6546\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.4s\n",
      "18/18 - 3s - 157ms/step - accuracy: 0.0122 - loss: 0.6745 - val_accuracy: 0.0000e+00 - val_loss: 0.6411\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "27/27 - 3s - 109ms/step - accuracy: 0.1997 - loss: 0.5303 - val_accuracy: 0.9698 - val_loss: 0.3511\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "36/36 - 3s - 81ms/step - accuracy: 0.0118 - loss: 0.5869 - val_accuracy: 0.0000e+00 - val_loss: 0.4199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "45/45 - 3s - 70ms/step - accuracy: 0.2259 - loss: 0.5121 - val_accuracy: 0.9874 - val_loss: 0.2652\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "9/9 - 3s - 295ms/step - accuracy: 0.0245 - loss: 0.6655 - val_accuracy: 0.0000e+00 - val_loss: 0.6131\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.3s\n",
      "18/18 - 3s - 158ms/step - accuracy: 0.0314 - loss: 0.6285 - val_accuracy: 0.0000e+00 - val_loss: 0.5801\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 4s - 143ms/step - accuracy: 0.0210 - loss: 0.5958 - val_accuracy: 0.0000e+00 - val_loss: 0.4668\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=25, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.4s\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [25, 50, 75]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params = []\n",
    "best_score = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[categorical])\n",
    "\n",
    "    # preprocess data\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 1)\n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 1)\n",
    "\n",
    "    for conv in conv_layer: \n",
    "        \"\"\"will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\"\"\"\n",
    "        \n",
    "        if conv and X.ndim != 4: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [1],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"binary_crossentropy\"],\n",
    "        'metrics': [[\"accuracy\"]],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], \n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [True],\n",
    "        'epochs': [30, 50],\n",
    "        'batch':[32,64],\n",
    "        }\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter = 20,  \n",
    "            scoring = make_scorer(custom_accuracy), \n",
    "            cv=tscv,  \n",
    "            verbose=2,  \n",
    "            n_jobs= 1,  \n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params.append(random_search.best_params_)\n",
    "        best_score.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 25 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 25, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.1, 'conv_layer': False, 'conv_act': 'relu', 'classification': True, 'batch': 64, 'activation': 'tanh'}\n",
      "Best Score for 25 npast_timesteps: 0.8999931029726188\n"
     ]
    }
   ],
   "source": [
    "# # take the best score and find associated parameters\n",
    "\n",
    "max_index = best_score.index(max(best_score))\n",
    "print(f'Best Params for 25 npast_timesteps: {best_params[max_index]}')\n",
    "print(f'Best Score for 25 npast_timesteps: {max(best_score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to list\n",
    "best_scores_ = []\n",
    "best_params_ = []\n",
    "\n",
    "best_scores_.append(max(best_score))\n",
    "best_params_.append(best_params[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 npast_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 11:40:27.265088: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 3s - 201ms/step - accuracy: 0.0152 - loss: 0.5844 - val_accuracy: 0.0000e+00 - val_loss: 0.4806\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 4s - 166ms/step - accuracy: 0.0772 - loss: 0.5252 - val_accuracy: 0.9696 - val_loss: 0.4026\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "41/41 - 6s - 134ms/step - accuracy: 0.4499 - loss: 0.4265 - val_accuracy: 0.9438 - val_loss: 0.3607\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 4s - 289ms/step - accuracy: 0.0631 - loss: 0.6648 - val_accuracy: 0.2477 - val_loss: 0.6144\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 4s - 144ms/step - accuracy: 0.0070 - loss: 0.6617 - val_accuracy: 0.0000e+00 - val_loss: 0.5780\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "41/41 - 5s - 115ms/step - accuracy: 0.0258 - loss: 0.6467 - val_accuracy: 0.0000e+00 - val_loss: 0.5959\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "14/14 - 6s - 394ms/step - accuracy: 0.0047 - loss: 0.6326 - val_accuracy: 0.0047 - val_loss: 0.4909\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.4s\n",
      "27/27 - 7s - 272ms/step - accuracy: 0.3915 - loss: 0.5568 - val_accuracy: 0.9696 - val_loss: 0.5293\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.3s\n",
      "41/41 - 10s - 248ms/step - accuracy: 0.5630 - loss: 0.5115 - val_accuracy: 0.9438 - val_loss: 0.5096\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.1s\n",
      "14/14 - 3s - 229ms/step - accuracy: 0.0012 - loss: 0.6235 - val_accuracy: 0.0000e+00 - val_loss: 0.5380\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 152ms/step - accuracy: 0.0023 - loss: 0.5655 - val_accuracy: 0.0000e+00 - val_loss: 0.4439\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "41/41 - 5s - 119ms/step - accuracy: 0.0449 - loss: 0.5031 - val_accuracy: 0.0000e+00 - val_loss: 0.3815\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "14/14 - 4s - 263ms/step - accuracy: 0.8271 - loss: 0.6786 - val_accuracy: 0.9626 - val_loss: 0.6539\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 4s - 150ms/step - accuracy: 0.0129 - loss: 0.6392 - val_accuracy: 0.0000e+00 - val_loss: 0.4896\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "41/41 - 5s - 119ms/step - accuracy: 0.0851 - loss: 0.6145 - val_accuracy: 0.0000e+00 - val_loss: 2.5450\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "14/14 - 3s - 238ms/step - accuracy: 0.0023 - loss: 0.5999 - val_accuracy: 0.0047 - val_loss: 0.4598\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 4s - 160ms/step - accuracy: 0.2598 - loss: 0.5317 - val_accuracy: 0.9696 - val_loss: 0.3897\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "41/41 - 6s - 148ms/step - accuracy: 0.6871 - loss: 0.4304 - val_accuracy: 0.9438 - val_loss: 0.3780\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "14/14 - 5s - 375ms/step - accuracy: 0.5678 - loss: 0.5070 - val_accuracy: 0.9626 - val_loss: 0.4094\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "27/27 - 8s - 289ms/step - accuracy: 0.3657 - loss: 0.4357 - val_accuracy: 0.9696 - val_loss: 0.3266\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.8s\n",
      "41/41 - 9s - 226ms/step - accuracy: 0.7082 - loss: 0.4304 - val_accuracy: 0.9438 - val_loss: 0.3503\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  11.3s\n",
      "14/14 - 6s - 428ms/step - accuracy: 0.2290 - loss: 0.6681 - val_accuracy: 0.9626 - val_loss: 0.5736\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.0s\n",
      "27/27 - 8s - 289ms/step - accuracy: 0.0135 - loss: 0.5708 - val_accuracy: 0.0000e+00 - val_loss: 0.4409\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.9s\n",
      "41/41 - 9s - 227ms/step - accuracy: 0.3863 - loss: 0.5171 - val_accuracy: 0.9438 - val_loss: 0.3725\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  11.2s\n",
      "14/14 - 5s - 380ms/step - accuracy: 0.0035 - loss: 0.5675 - val_accuracy: 0.0047 - val_loss: 0.4640\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "27/27 - 7s - 270ms/step - accuracy: 0.8455 - loss: 0.4368 - val_accuracy: 0.9696 - val_loss: 0.3420\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.4s\n",
      "41/41 - 11s - 267ms/step - accuracy: 0.4105 - loss: 0.4336 - val_accuracy: 0.9438 - val_loss: 0.3545\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  13.0s\n",
      "14/14 - 3s - 245ms/step - accuracy: 0.0058 - loss: 0.6505 - val_accuracy: 0.0000e+00 - val_loss: 0.5425\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "27/27 - 4s - 163ms/step - accuracy: 0.0217 - loss: 0.6158 - val_accuracy: 0.0000e+00 - val_loss: 0.4648\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "41/41 - 6s - 135ms/step - accuracy: 0.4187 - loss: 0.5428 - val_accuracy: 0.8768 - val_loss: 0.5107\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 4s - 276ms/step - accuracy: 0.4614 - loss: 0.6699 - val_accuracy: 0.9626 - val_loss: 0.6304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 4s - 153ms/step - accuracy: 0.6811 - loss: 0.6959 - val_accuracy: 0.9696 - val_loss: 0.5675\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "41/41 - 5s - 119ms/step - accuracy: 0.7140 - loss: 0.5916 - val_accuracy: 0.6849 - val_loss: 0.5426\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "14/14 - 5s - 322ms/step - accuracy: 0.0900 - loss: 0.6485 - val_accuracy: 0.0000e+00 - val_loss: 0.5606\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 5s - 173ms/step - accuracy: 0.5448 - loss: 0.5456 - val_accuracy: 0.9696 - val_loss: 0.4052\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "41/41 - 6s - 137ms/step - accuracy: 0.7917 - loss: 0.4556 - val_accuracy: 0.9438 - val_loss: 0.3614\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.9s\n",
      "14/14 - 5s - 359ms/step - accuracy: 0.4498 - loss: 0.5728 - val_accuracy: 0.9626 - val_loss: 0.4522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "27/27 - 5s - 180ms/step - accuracy: 0.0123 - loss: 0.5584 - val_accuracy: 0.0000e+00 - val_loss: 0.4130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "41/41 - 8s - 202ms/step - accuracy: 0.8369 - loss: 0.4390 - val_accuracy: 0.9438 - val_loss: 0.3604\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.5s\n",
      "14/14 - 5s - 324ms/step - accuracy: 0.2091 - loss: 0.6588 - val_accuracy: 0.9626 - val_loss: 0.5689\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "27/27 - 7s - 273ms/step - accuracy: 0.4745 - loss: 0.6206 - val_accuracy: 0.9696 - val_loss: 0.4935\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.0s\n",
      "41/41 - 8s - 193ms/step - accuracy: 0.5876 - loss: 0.6007 - val_accuracy: 0.9438 - val_loss: 0.4365\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  11.5s\n",
      "14/14 - 6s - 408ms/step - accuracy: 0.0105 - loss: 0.6235 - val_accuracy: 0.0000e+00 - val_loss: 0.5271\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.7s\n",
      "27/27 - 6s - 235ms/step - accuracy: 0.2510 - loss: 0.4776 - val_accuracy: 0.9696 - val_loss: 0.3226\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.0s\n",
      "41/41 - 8s - 184ms/step - accuracy: 0.5197 - loss: 0.4199 - val_accuracy: 0.9438 - val_loss: 0.3639\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.1s\n",
      "14/14 - 4s - 274ms/step - accuracy: 0.2255 - loss: 0.6790 - val_accuracy: 0.2897 - val_loss: 0.6552\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 5s - 181ms/step - accuracy: 0.0252 - loss: 0.6283 - val_accuracy: 0.0304 - val_loss: 0.5552\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "41/41 - 6s - 152ms/step - accuracy: 0.0105 - loss: 0.6075 - val_accuracy: 0.0000e+00 - val_loss: 0.5684\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.6s\n",
      "14/14 - 8s - 540ms/step - accuracy: 0.0689 - loss: 0.4872 - val_accuracy: 0.9626 - val_loss: 0.3607\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.2s\n",
      "27/27 - 12s - 444ms/step - accuracy: 0.2756 - loss: 0.4205 - val_accuracy: 0.9696 - val_loss: 0.3198\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  15.6s\n",
      "41/41 - 14s - 347ms/step - accuracy: 0.6902 - loss: 0.4302 - val_accuracy: 0.9438 - val_loss: 0.3455\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  17.5s\n",
      "14/14 - 6s - 450ms/step - accuracy: 0.8598 - loss: 0.6652 - val_accuracy: 0.9626 - val_loss: 0.5970\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.3s\n",
      "27/27 - 6s - 240ms/step - accuracy: 0.0199 - loss: 0.6117 - val_accuracy: 0.0350 - val_loss: 0.4447\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "41/41 - 8s - 190ms/step - accuracy: 0.2758 - loss: 0.5317 - val_accuracy: 0.9438 - val_loss: 0.3738\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.0s\n",
      "14/14 - 5s - 380ms/step - accuracy: 0.0245 - loss: 0.6866 - val_accuracy: 0.0000e+00 - val_loss: 0.6760\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "27/27 - 6s - 237ms/step - accuracy: 0.0345 - loss: 0.5948 - val_accuracy: 0.0304 - val_loss: 0.5484\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.5s\n",
      "41/41 - 8s - 202ms/step - accuracy: 0.4093 - loss: 0.5578 - val_accuracy: 0.9438 - val_loss: 0.4664\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.3s\n",
      "14/14 - 5s - 335ms/step - accuracy: 0.0596 - loss: 0.6343 - val_accuracy: 0.9626 - val_loss: 0.5355\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "27/27 - 6s - 229ms/step - accuracy: 0.8561 - loss: 0.4768 - val_accuracy: 0.9696 - val_loss: 0.3523\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "41/41 - 8s - 199ms/step - accuracy: 0.1081 - loss: 0.4345 - val_accuracy: 0.0000e+00 - val_loss: 0.3718\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 18s - 324ms/step - accuracy: 0.5451 - loss: 0.4980 - val_accuracy: 0.0000e+00 - val_loss: 0.4279\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "14/14 - 4s - 287ms/step - accuracy: 0.5993 - loss: 0.6167 - val_accuracy: 0.9626 - val_loss: 0.5446\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "27/27 - 6s - 224ms/step - accuracy: 0.1217 - loss: 0.5344 - val_accuracy: 0.0000e+00 - val_loss: 0.4179\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.4s\n",
      "41/41 - 5s - 120ms/step - accuracy: 0.1268 - loss: 0.5108 - val_accuracy: 0.0000e+00 - val_loss: 0.4218\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "14/14 - 4s - 266ms/step - accuracy: 0.0093 - loss: 0.6533 - val_accuracy: 0.0000e+00 - val_loss: 0.5600\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 4s - 134ms/step - accuracy: 0.6167 - loss: 0.6202 - val_accuracy: 0.9696 - val_loss: 0.5137\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "41/41 - 4s - 105ms/step - accuracy: 0.4042 - loss: 0.5233 - val_accuracy: 0.9438 - val_loss: 0.3687\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "14/14 - 4s - 311ms/step - accuracy: 0.6519 - loss: 0.6869 - val_accuracy: 0.9626 - val_loss: 0.6676\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "27/27 - 5s - 180ms/step - accuracy: 0.0012 - loss: 0.6144 - val_accuracy: 0.0000e+00 - val_loss: 0.4336\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "41/41 - 5s - 121ms/step - accuracy: 0.1069 - loss: 0.5404 - val_accuracy: 0.9438 - val_loss: 0.3760\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "14/14 - 4s - 274ms/step - accuracy: 0.1659 - loss: 0.6581 - val_accuracy: 0.9626 - val_loss: 0.6115\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 4s - 138ms/step - accuracy: 0.0099 - loss: 0.6101 - val_accuracy: 0.0304 - val_loss: 0.5380\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "41/41 - 4s - 102ms/step - accuracy: 0.9376 - loss: 0.5670 - val_accuracy: 0.9438 - val_loss: 0.4440\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "14/14 - 3s - 228ms/step - accuracy: 0.0678 - loss: 0.6857 - val_accuracy: 0.0000e+00 - val_loss: 0.6670\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "27/27 - 4s - 140ms/step - accuracy: 0.1556 - loss: 0.5453 - val_accuracy: 0.0000e+00 - val_loss: 0.3693\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "41/41 - 4s - 91ms/step - accuracy: 0.0055 - loss: 0.5732 - val_accuracy: 0.0000e+00 - val_loss: 0.4447\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "14/14 - 4s - 257ms/step - accuracy: 0.6285 - loss: 0.6058 - val_accuracy: 0.9626 - val_loss: 0.5095\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "27/27 - 4s - 138ms/step - accuracy: 0.1539 - loss: 0.5466 - val_accuracy: 0.9696 - val_loss: 0.4221\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "41/41 - 5s - 111ms/step - accuracy: 0.3730 - loss: 0.4836 - val_accuracy: 0.9438 - val_loss: 0.3755\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "14/14 - 5s - 342ms/step - accuracy: 0.0210 - loss: 0.6564 - val_accuracy: 0.0000e+00 - val_loss: 0.5955\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "27/27 - 5s - 176ms/step - accuracy: 0.4857 - loss: 0.6025 - val_accuracy: 0.9696 - val_loss: 0.4838\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "41/41 - 5s - 123ms/step - accuracy: 0.7035 - loss: 0.5295 - val_accuracy: 0.9438 - val_loss: 0.3892\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "14/14 - 5s - 326ms/step - accuracy: 0.0269 - loss: 0.6827 - val_accuracy: 0.0000e+00 - val_loss: 0.6592\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "27/27 - 5s - 178ms/step - accuracy: 0.1521 - loss: 0.6169 - val_accuracy: 0.9696 - val_loss: 0.3825\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "41/41 - 7s - 182ms/step - accuracy: 0.0675 - loss: 0.5825 - val_accuracy: 0.0000e+00 - val_loss: 0.4498\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "14/14 - 5s - 338ms/step - accuracy: 0.0000e+00 - loss: 0.6426 - val_accuracy: 0.0000e+00 - val_loss: 0.5743\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.6s\n",
      "27/27 - 5s - 197ms/step - accuracy: 0.0509 - loss: 0.6229 - val_accuracy: 0.9696 - val_loss: 0.5223\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "41/41 - 5s - 133ms/step - accuracy: 0.0066 - loss: 0.5535 - val_accuracy: 0.0000e+00 - val_loss: 0.4343\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.8s\n",
      "14/14 - 4s - 310ms/step - accuracy: 0.0012 - loss: 0.6451 - val_accuracy: 0.0000e+00 - val_loss: 0.5842\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "27/27 - 4s - 134ms/step - accuracy: 0.0088 - loss: 0.5947 - val_accuracy: 0.0000e+00 - val_loss: 0.4431\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "41/41 - 4s - 92ms/step - accuracy: 0.1744 - loss: 0.5108 - val_accuracy: 0.9438 - val_loss: 0.4156\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "14/14 - 3s - 234ms/step - accuracy: 0.0023 - loss: 0.6790 - val_accuracy: 0.0000e+00 - val_loss: 0.6575\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 3s - 129ms/step - accuracy: 0.2551 - loss: 0.5719 - val_accuracy: 0.9509 - val_loss: 0.3356\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "41/41 - 3s - 82ms/step - accuracy: 0.2208 - loss: 0.5170 - val_accuracy: 0.4384 - val_loss: 0.3790\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "14/14 - 3s - 227ms/step - accuracy: 0.0315 - loss: 0.6050 - val_accuracy: 0.0047 - val_loss: 0.5083\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "27/27 - 4s - 136ms/step - accuracy: 0.0468 - loss: 0.5946 - val_accuracy: 0.0000e+00 - val_loss: 0.4642\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "41/41 - 4s - 94ms/step - accuracy: 0.4339 - loss: 0.4915 - val_accuracy: 0.9438 - val_loss: 0.3997\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "14/14 - 3s - 239ms/step - accuracy: 0.0129 - loss: 0.6280 - val_accuracy: 0.0327 - val_loss: 0.5492\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 133ms/step - accuracy: 0.0012 - loss: 0.5779 - val_accuracy: 0.0000e+00 - val_loss: 0.4550\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "41/41 - 4s - 89ms/step - accuracy: 0.0835 - loss: 0.5125 - val_accuracy: 0.0000e+00 - val_loss: 0.3871\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "14/14 - 3s - 238ms/step - accuracy: 0.0140 - loss: 0.6119 - val_accuracy: 0.0000e+00 - val_loss: 0.4918\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 136ms/step - accuracy: 0.1960 - loss: 0.5447 - val_accuracy: 0.9696 - val_loss: 0.3637\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.7s\n",
      "41/41 - 5s - 115ms/step - accuracy: 0.2197 - loss: 0.5120 - val_accuracy: 0.9438 - val_loss: 0.3934\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "14/14 - 4s - 253ms/step - accuracy: 0.5794 - loss: 0.6252 - val_accuracy: 0.9626 - val_loss: 0.5397\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "27/27 - 4s - 137ms/step - accuracy: 0.1404 - loss: 0.5850 - val_accuracy: 0.0304 - val_loss: 0.4814\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "41/41 - 4s - 95ms/step - accuracy: 0.1803 - loss: 0.4827 - val_accuracy: 0.0000e+00 - val_loss: 0.3972\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "14/14 - 3s - 238ms/step - accuracy: 0.0105 - loss: 0.6770 - val_accuracy: 0.0000e+00 - val_loss: 0.6578\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 4s - 150ms/step - accuracy: 0.0076 - loss: 0.6601 - val_accuracy: 0.0000e+00 - val_loss: 0.5737\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "41/41 - 6s - 142ms/step - accuracy: 0.1896 - loss: 0.5761 - val_accuracy: 0.0530 - val_loss: 0.4241\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.9s\n",
      "14/14 - 5s - 355ms/step - accuracy: 0.0350 - loss: 0.6495 - val_accuracy: 0.0047 - val_loss: 0.5619\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "27/27 - 5s - 183ms/step - accuracy: 0.5120 - loss: 0.6171 - val_accuracy: 0.9696 - val_loss: 0.4964\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "41/41 - 5s - 130ms/step - accuracy: 0.0671 - loss: 0.5257 - val_accuracy: 0.0000e+00 - val_loss: 0.4127\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 3s - 238ms/step - accuracy: 0.0117 - loss: 0.6764 - val_accuracy: 0.0000e+00 - val_loss: 0.6413\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 135ms/step - accuracy: 0.0380 - loss: 0.5800 - val_accuracy: 0.9696 - val_loss: 0.4270\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "41/41 - 4s - 106ms/step - accuracy: 0.4128 - loss: 0.5200 - val_accuracy: 0.9438 - val_loss: 0.3891\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "14/14 - 3s - 234ms/step - accuracy: 0.6460 - loss: 0.5983 - val_accuracy: 0.9626 - val_loss: 0.3553\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 4s - 138ms/step - accuracy: 0.1884 - loss: 0.5329 - val_accuracy: 0.9696 - val_loss: 0.3516\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "41/41 - 4s - 99ms/step - accuracy: 0.0456 - loss: 0.5498 - val_accuracy: 0.0000e+00 - val_loss: 0.3988\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "14/14 - 4s - 264ms/step - accuracy: 0.0000e+00 - loss: 0.6144 - val_accuracy: 0.0047 - val_loss: 0.5182\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 5s - 172ms/step - accuracy: 0.1147 - loss: 0.5624 - val_accuracy: 0.9696 - val_loss: 0.4350\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "41/41 - 12s - 302ms/step - accuracy: 0.1198 - loss: 0.5046 - val_accuracy: 0.9438 - val_loss: 0.4132\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 6s - 114ms/step - accuracy: 0.6915 - loss: 0.5315 - val_accuracy: 0.9673 - val_loss: 0.3426\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [50]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params_50 = []\n",
    "best_score_50 = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[categorical])\n",
    "\n",
    "    # preprocess data\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 1)\n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 1)\n",
    "\n",
    "    for conv in conv_layer: \n",
    "        \"\"\"will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\"\"\"\n",
    "        \n",
    "        if conv and X.ndim != 4: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [1],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"binary_crossentropy\"],\n",
    "        'metrics': [[\"accuracy\"]],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], \n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [True],\n",
    "        'epochs': [30, 50],\n",
    "        'batch':[32,64],\n",
    "        }\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter=20,  # Number of different combinations to sample\n",
    "            scoring = make_scorer(custom_accuracy), \n",
    "            cv=tscv,  # Number of cross-validation folds\n",
    "            verbose=2,  # Display search progress\n",
    "            n_jobs= 1,  # Use all available CPUs\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params_50.append(random_search.best_params_)\n",
    "        best_score_50.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 50 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 32, 'neurons': [128, 64, 32], 'n_timesteps': 50, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.1, 'conv_layer': False, 'conv_act': 'tanh', 'classification': True, 'batch': 64, 'activation': 'relu'}\n",
      "Best Scorefor 50 npast_timesteps: 0.8993370127390746\n"
     ]
    }
   ],
   "source": [
    "# take the best score and find associated parameters\n",
    "\n",
    "max_index_50 = best_score_50.index(max(best_score_50))\n",
    "print(f'Best Params for 50 npast_timesteps: {best_params_50[max_index_50]}')\n",
    "print(f'Best Scorefor 50 npast_timesteps: {max(best_score_50)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores_.append(max(best_score_50))\n",
    "best_params_.append(best_params_50[max_index_50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75 npast_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 11:55:28.332335: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 6s - 423ms/step - accuracy: 0.6694 - loss: 0.5869 - val_accuracy: 0.9624 - val_loss: 0.4511\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.2s\n",
      "27/27 - 7s - 269ms/step - accuracy: 0.6492 - loss: 0.5025 - val_accuracy: 0.9694 - val_loss: 0.3635\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.1s\n",
      "40/40 - 9s - 230ms/step - accuracy: 0.6064 - loss: 0.4332 - val_accuracy: 0.9498 - val_loss: 0.3569\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.7s\n",
      "14/14 - 5s - 340ms/step - accuracy: 0.4612 - loss: 0.6461 - val_accuracy: 0.9624 - val_loss: 0.5818\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.8s\n",
      "27/27 - 8s - 314ms/step - accuracy: 0.0059 - loss: 0.7065 - val_accuracy: 0.0000e+00 - val_loss: 1.4971\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.8s\n",
      "40/40 - 8s - 205ms/step - accuracy: 0.0381 - loss: 0.5424 - val_accuracy: 0.0000e+00 - val_loss: 0.4981\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.3s\n",
      "14/14 - 9s - 613ms/step - accuracy: 0.0035 - loss: 0.6641 - val_accuracy: 0.0000e+00 - val_loss: 0.7101\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  11.3s\n",
      "27/27 - 12s - 460ms/step - accuracy: 0.0082 - loss: 0.6846 - val_accuracy: 0.0000e+00 - val_loss: 0.5589\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  15.7s\n",
      "40/40 - 16s - 389ms/step - accuracy: 0.0510 - loss: 0.6416 - val_accuracy: 0.5479 - val_loss: 0.8919\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  19.1s\n",
      "14/14 - 5s - 372ms/step - accuracy: 0.3118 - loss: 0.6349 - val_accuracy: 0.9624 - val_loss: 0.5475\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.9s\n",
      "27/27 - 6s - 232ms/step - accuracy: 0.0018 - loss: 0.5271 - val_accuracy: 0.0000e+00 - val_loss: 0.3817\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.1s\n",
      "40/40 - 8s - 201ms/step - accuracy: 0.0389 - loss: 0.4808 - val_accuracy: 0.0000e+00 - val_loss: 0.3840\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.9s\n",
      "14/14 - 5s - 365ms/step - accuracy: 0.1706 - loss: 0.6792 - val_accuracy: 0.0329 - val_loss: 0.6623\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.8s\n",
      "27/27 - 7s - 244ms/step - accuracy: 0.3384 - loss: 0.6597 - val_accuracy: 0.8706 - val_loss: 0.7717\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.1s\n",
      "40/40 - 7s - 185ms/step - accuracy: 0.6197 - loss: 0.5960 - val_accuracy: 0.9498 - val_loss: 0.4985\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "14/14 - 4s - 320ms/step - accuracy: 0.4082 - loss: 0.5827 - val_accuracy: 0.9624 - val_loss: 0.4836\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "27/27 - 6s - 204ms/step - accuracy: 0.2084 - loss: 0.5019 - val_accuracy: 0.9694 - val_loss: 0.3818\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "40/40 - 8s - 208ms/step - accuracy: 0.3002 - loss: 0.4413 - val_accuracy: 0.9498 - val_loss: 0.3598\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.8s\n",
      "14/14 - 7s - 466ms/step - accuracy: 0.0000e+00 - loss: 0.5188 - val_accuracy: 0.0047 - val_loss: 0.3978\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.3s\n",
      "27/27 - 10s - 353ms/step - accuracy: 0.1177 - loss: 0.4558 - val_accuracy: 0.9694 - val_loss: 0.3651\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.3s\n",
      "40/40 - 13s - 329ms/step - accuracy: 0.1154 - loss: 0.4204 - val_accuracy: 0.0000e+00 - val_loss: 0.3682\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  15.8s\n",
      "14/14 - 7s - 510ms/step - accuracy: 0.0600 - loss: 0.6592 - val_accuracy: 0.9624 - val_loss: 0.5760\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.8s\n",
      "27/27 - 9s - 350ms/step - accuracy: 0.3284 - loss: 0.5877 - val_accuracy: 0.9694 - val_loss: 0.4595\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.0s\n",
      "40/40 - 13s - 328ms/step - accuracy: 0.6715 - loss: 0.5079 - val_accuracy: 0.9482 - val_loss: 0.7566\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  15.6s\n",
      "14/14 - 6s - 463ms/step - accuracy: 0.1588 - loss: 0.5100 - val_accuracy: 0.0047 - val_loss: 0.3973\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.3s\n",
      "27/27 - 10s - 353ms/step - accuracy: 0.0194 - loss: 0.4687 - val_accuracy: 0.0000e+00 - val_loss: 0.3745\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.8s\n",
      "40/40 - 17s - 430ms/step - accuracy: 0.2331 - loss: 0.4234 - val_accuracy: 0.9498 - val_loss: 0.3649\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  26.4s\n",
      "14/14 - 5s - 359ms/step - accuracy: 0.2247 - loss: 0.6265 - val_accuracy: 0.0047 - val_loss: 0.3700\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.8s\n",
      "27/27 - 7s - 248ms/step - accuracy: 0.3761 - loss: 0.6106 - val_accuracy: 0.9694 - val_loss: 0.5738\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.2s\n",
      "40/40 - 8s - 203ms/step - accuracy: 0.0043 - loss: 0.6447 - val_accuracy: 0.0000e+00 - val_loss: 0.6007\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.7s\n",
      "14/14 - 4s - 306ms/step - accuracy: 0.1494 - loss: 0.6189 - val_accuracy: 0.0000e+00 - val_loss: 0.5058\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "27/27 - 6s - 240ms/step - accuracy: 0.1377 - loss: 1.1113 - val_accuracy: 0.6400 - val_loss: 3.0970\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "40/40 - 7s - 175ms/step - accuracy: 0.4003 - loss: 0.5833 - val_accuracy: 0.8885 - val_loss: 0.5009\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "14/14 - 5s - 365ms/step - accuracy: 0.1788 - loss: 0.5653 - val_accuracy: 0.9624 - val_loss: 0.4342\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "27/27 - 6s - 214ms/step - accuracy: 0.0283 - loss: 0.5164 - val_accuracy: 0.0000e+00 - val_loss: 0.3843\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "40/40 - 7s - 181ms/step - accuracy: 0.5911 - loss: 0.4532 - val_accuracy: 0.9498 - val_loss: 0.3559\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.3s\n",
      "14/14 - 4s - 285ms/step - accuracy: 0.0212 - loss: 0.5918 - val_accuracy: 0.0047 - val_loss: 0.4684\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 6s - 217ms/step - accuracy: 0.0135 - loss: 0.5175 - val_accuracy: 0.0000e+00 - val_loss: 0.3677\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.3s\n",
      "40/40 - 7s - 180ms/step - accuracy: 0.8783 - loss: 0.4216 - val_accuracy: 0.9498 - val_loss: 0.3452\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.6s\n",
      "14/14 - 4s - 287ms/step - accuracy: 0.1376 - loss: 0.6659 - val_accuracy: 0.0329 - val_loss: 0.6118\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 6s - 230ms/step - accuracy: 0.5533 - loss: 0.5720 - val_accuracy: 0.9694 - val_loss: 0.5197\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.6s\n",
      "40/40 - 7s - 176ms/step - accuracy: 0.0012 - loss: 0.5725 - val_accuracy: 0.0000e+00 - val_loss: 0.4854\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.7s\n",
      "14/14 - 4s - 294ms/step - accuracy: 0.0000e+00 - loss: 0.5896 - val_accuracy: 0.0047 - val_loss: 0.4815\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "27/27 - 7s - 247ms/step - accuracy: 0.1360 - loss: 0.5319 - val_accuracy: 0.9694 - val_loss: 0.4118\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.3s\n",
      "40/40 - 8s - 195ms/step - accuracy: 0.5051 - loss: 0.4704 - val_accuracy: 0.9498 - val_loss: 0.3634\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.2s\n",
      "14/14 - 4s - 266ms/step - accuracy: 0.0082 - loss: 0.6779 - val_accuracy: 0.0000e+00 - val_loss: 0.6613\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 5s - 187ms/step - accuracy: 0.0141 - loss: 0.6274 - val_accuracy: 0.0000e+00 - val_loss: 0.5495\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.4s\n",
      "40/40 - 6s - 162ms/step - accuracy: 0.0110 - loss: 0.6366 - val_accuracy: 0.1915 - val_loss: 0.5847\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "14/14 - 6s - 459ms/step - accuracy: 0.1247 - loss: 0.4987 - val_accuracy: 0.0047 - val_loss: 0.3662\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.2s\n",
      "27/27 - 10s - 372ms/step - accuracy: 0.0459 - loss: 0.4810 - val_accuracy: 0.0000e+00 - val_loss: 0.3688\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.9s\n",
      "40/40 - 13s - 313ms/step - accuracy: 0.6193 - loss: 0.3784 - val_accuracy: 0.9498 - val_loss: 0.3697\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  16.7s\n",
      "14/14 - 5s - 335ms/step - accuracy: 0.0518 - loss: 0.6630 - val_accuracy: 0.0047 - val_loss: 0.5842\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "27/27 - 6s - 214ms/step - accuracy: 0.9417 - loss: 0.5787 - val_accuracy: 0.9694 - val_loss: 0.4560\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "40/40 - 7s - 182ms/step - accuracy: 0.2284 - loss: 0.6187 - val_accuracy: 0.6672 - val_loss: 0.4601\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "14/14 - 4s - 288ms/step - accuracy: 0.0635 - loss: 0.6540 - val_accuracy: 0.0141 - val_loss: 0.5769\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 6s - 209ms/step - accuracy: 0.1189 - loss: 0.6438 - val_accuracy: 0.0353 - val_loss: 0.5718\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "40/40 - 8s - 201ms/step - accuracy: 0.0420 - loss: 0.6787 - val_accuracy: 0.6876 - val_loss: 0.5842\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.6s\n",
      "14/14 - 4s - 281ms/step - accuracy: 0.6541 - loss: 0.5706 - val_accuracy: 0.9624 - val_loss: 0.4394\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "27/27 - 6s - 228ms/step - accuracy: 5.8858e-04 - loss: 0.5251 - val_accuracy: 0.0000e+00 - val_loss: 0.3867\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.0s\n",
      "40/40 - 8s - 195ms/step - accuracy: 0.5589 - loss: 0.4320 - val_accuracy: 0.9498 - val_loss: 0.3581\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=False, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 17s - 317ms/step - accuracy: 0.2356 - loss: 0.6013 - val_accuracy: 0.9671 - val_loss: 0.5133\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "14/14 - 3s - 224ms/step - accuracy: 0.7753 - loss: 0.6218 - val_accuracy: 0.9624 - val_loss: 0.5435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "27/27 - 5s - 185ms/step - accuracy: 0.5921 - loss: 0.4843 - val_accuracy: 0.9694 - val_loss: 0.3808\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "40/40 - 4s - 100ms/step - accuracy: 0.0620 - loss: 0.5247 - val_accuracy: 0.0000e+00 - val_loss: 0.4459\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "14/14 - 3s - 224ms/step - accuracy: 0.2753 - loss: 0.6689 - val_accuracy: 0.9624 - val_loss: 0.6050\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "27/27 - 3s - 122ms/step - accuracy: 0.4974 - loss: 0.6097 - val_accuracy: 0.9694 - val_loss: 0.5016\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "40/40 - 3s - 80ms/step - accuracy: 0.1845 - loss: 0.5190 - val_accuracy: 0.9498 - val_loss: 0.3663\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "14/14 - 4s - 286ms/step - accuracy: 0.0329 - loss: 0.6715 - val_accuracy: 0.0000e+00 - val_loss: 0.5742\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "27/27 - 4s - 153ms/step - accuracy: 0.8005 - loss: 0.5748 - val_accuracy: 0.9694 - val_loss: 0.6346\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "40/40 - 5s - 113ms/step - accuracy: 0.3666 - loss: 0.4834 - val_accuracy: 0.9498 - val_loss: 0.3806\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "14/14 - 3s - 201ms/step - accuracy: 0.7118 - loss: 0.6209 - val_accuracy: 0.9624 - val_loss: 0.5542\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 108ms/step - accuracy: 0.0188 - loss: 0.6058 - val_accuracy: 0.0000e+00 - val_loss: 0.5282\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "40/40 - 3s - 77ms/step - accuracy: 0.7936 - loss: 0.5501 - val_accuracy: 0.9498 - val_loss: 0.4504\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "14/14 - 5s - 350ms/step - accuracy: 0.2671 - loss: 0.6162 - val_accuracy: 0.9624 - val_loss: 0.4953\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "27/27 - 3s - 119ms/step - accuracy: 0.3773 - loss: 0.5571 - val_accuracy: 0.9694 - val_loss: 0.4194\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 4s - 90ms/step - accuracy: 0.1197 - loss: 0.4882 - val_accuracy: 0.0000e+00 - val_loss: 0.4065\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "14/14 - 3s - 208ms/step - accuracy: 0.0176 - loss: 0.6136 - val_accuracy: 0.0000e+00 - val_loss: 0.5275\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 119ms/step - accuracy: 0.6992 - loss: 0.5480 - val_accuracy: 0.9694 - val_loss: 0.4360\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 3s - 87ms/step - accuracy: 0.6036 - loss: 0.5036 - val_accuracy: 0.9498 - val_loss: 0.3918\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "14/14 - 4s - 313ms/step - accuracy: 0.2741 - loss: 0.6438 - val_accuracy: 0.9624 - val_loss: 0.5687\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 4s - 149ms/step - accuracy: 0.3408 - loss: 0.5729 - val_accuracy: 0.0000e+00 - val_loss: 0.4260\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "40/40 - 5s - 114ms/step - accuracy: 0.9195 - loss: 0.5049 - val_accuracy: 0.9498 - val_loss: 0.3689\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "14/14 - 4s - 284ms/step - accuracy: 0.1882 - loss: 0.6862 - val_accuracy: 0.9624 - val_loss: 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 5s - 169ms/step - accuracy: 0.0371 - loss: 0.6358 - val_accuracy: 0.9694 - val_loss: 0.4040\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "40/40 - 5s - 116ms/step - accuracy: 0.0549 - loss: 0.5443 - val_accuracy: 0.0000e+00 - val_loss: 0.3915\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "14/14 - 8s - 553ms/step - accuracy: 0.7647 - loss: 0.6387 - val_accuracy: 0.9624 - val_loss: 0.5501\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.9s\n",
      "27/27 - 5s - 171ms/step - accuracy: 0.3673 - loss: 0.5888 - val_accuracy: 0.9694 - val_loss: 0.4424\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "40/40 - 6s - 138ms/step - accuracy: 0.4478 - loss: 0.4975 - val_accuracy: 0.9498 - val_loss: 0.3775\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 3s - 198ms/step - accuracy: 0.4553 - loss: 0.6564 - val_accuracy: 0.9624 - val_loss: 0.5386\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 116ms/step - accuracy: 0.1948 - loss: 0.5524 - val_accuracy: 0.9694 - val_loss: 0.3930\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 4s - 93ms/step - accuracy: 0.5789 - loss: 0.5104 - val_accuracy: 0.0000e+00 - val_loss: 0.4000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "14/14 - 4s - 279ms/step - accuracy: 0.0082 - loss: 0.6821 - val_accuracy: 0.0047 - val_loss: 0.6553\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 4s - 152ms/step - accuracy: 0.0594 - loss: 0.5865 - val_accuracy: 0.0000e+00 - val_loss: 0.3517\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "40/40 - 4s - 106ms/step - accuracy: 0.0145 - loss: 0.5614 - val_accuracy: 0.0000e+00 - val_loss: 0.4630\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "14/14 - 4s - 284ms/step - accuracy: 0.0565 - loss: 0.6170 - val_accuracy: 0.0000e+00 - val_loss: 0.5439\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 4s - 160ms/step - accuracy: 0.1660 - loss: 0.5655 - val_accuracy: 0.9694 - val_loss: 0.4571\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "40/40 - 4s - 111ms/step - accuracy: 0.5628 - loss: 0.4832 - val_accuracy: 0.9498 - val_loss: 0.3740\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.3, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "14/14 - 4s - 295ms/step - accuracy: 0.9071 - loss: 0.5916 - val_accuracy: 0.9624 - val_loss: 0.4794\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "27/27 - 4s - 130ms/step - accuracy: 0.2531 - loss: 0.5333 - val_accuracy: 0.9694 - val_loss: 0.4215\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "40/40 - 3s - 86ms/step - accuracy: 0.5506 - loss: 0.4815 - val_accuracy: 0.9498 - val_loss: 0.3970\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "14/14 - 3s - 238ms/step - accuracy: 0.0047 - loss: 0.6302 - val_accuracy: 0.0047 - val_loss: 0.4595\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 8s - 292ms/step - accuracy: 0.4491 - loss: 0.5403 - val_accuracy: 0.9694 - val_loss: 0.4000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.1s\n",
      "40/40 - 4s - 110ms/step - accuracy: 0.2688 - loss: 0.5592 - val_accuracy: 0.9498 - val_loss: 0.3768\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "14/14 - 4s - 289ms/step - accuracy: 0.0435 - loss: 0.6178 - val_accuracy: 0.0047 - val_loss: 0.5328\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 4s - 138ms/step - accuracy: 0.0035 - loss: 0.5724 - val_accuracy: 0.0000e+00 - val_loss: 0.4758\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "40/40 - 3s - 83ms/step - accuracy: 0.0510 - loss: 0.5128 - val_accuracy: 0.0000e+00 - val_loss: 0.4291\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "14/14 - 3s - 203ms/step - accuracy: 0.2506 - loss: 0.6746 - val_accuracy: 0.0000e+00 - val_loss: 0.6222\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 111ms/step - accuracy: 0.3726 - loss: 0.5351 - val_accuracy: 0.9694 - val_loss: 0.3970\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "40/40 - 4s - 94ms/step - accuracy: 0.2665 - loss: 0.5002 - val_accuracy: 0.7677 - val_loss: 0.4138\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "14/14 - 5s - 380ms/step - accuracy: 0.1259 - loss: 0.6646 - val_accuracy: 0.9624 - val_loss: 0.6131\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "27/27 - 5s - 186ms/step - accuracy: 0.3743 - loss: 0.5952 - val_accuracy: 0.9694 - val_loss: 0.4458\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "40/40 - 6s - 149ms/step - accuracy: 0.8026 - loss: 0.5461 - val_accuracy: 0.9498 - val_loss: 0.4129\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=32, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "14/14 - 4s - 304ms/step - accuracy: 0.1000 - loss: 0.6542 - val_accuracy: 0.0329 - val_loss: 0.5443\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "27/27 - 3s - 127ms/step - accuracy: 0.0012 - loss: 0.5759 - val_accuracy: 0.0000e+00 - val_loss: 0.3859\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "40/40 - 3s - 85ms/step - accuracy: 0.5173 - loss: 0.4625 - val_accuracy: 0.0078 - val_loss: 0.4027\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "14/14 - 4s - 278ms/step - accuracy: 0.0588 - loss: 0.6418 - val_accuracy: 0.0047 - val_loss: 0.4648\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "27/27 - 5s - 174ms/step - accuracy: 0.1012 - loss: 0.5887 - val_accuracy: 0.9694 - val_loss: 0.4939\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "40/40 - 4s - 103ms/step - accuracy: 0.1071 - loss: 0.5004 - val_accuracy: 0.9498 - val_loss: 0.4059\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "14/14 - 4s - 275ms/step - accuracy: 0.0165 - loss: 0.6012 - val_accuracy: 0.0000e+00 - val_loss: 0.5259\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 4s - 143ms/step - accuracy: 0.4915 - loss: 0.5194 - val_accuracy: 0.9694 - val_loss: 0.4185\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "40/40 - 8s - 197ms/step - accuracy: 0.7645 - loss: 0.5046 - val_accuracy: 0.9498 - val_loss: 0.4135\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=True, conv_act=relu, conv_layer=True, dropout=0.1, epochs=30, loss=binary_crossentropy, metrics=['accuracy'], n_features=7, n_predicted_timesteps=1, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 5s - 97ms/step - accuracy: 0.1428 - loss: 0.4902 - val_accuracy: 0.9671 - val_loss: 0.2961\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [75]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params_75 = []\n",
    "best_score_75 = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[categorical])\n",
    "\n",
    "    # preprocess data\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 1)\n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 1)\n",
    "\n",
    "    for conv in conv_layer: \n",
    "        \"\"\"will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\"\"\"\n",
    "        \n",
    "        if conv and X.ndim != 4: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [1],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"binary_crossentropy\"],\n",
    "        'metrics': [[\"accuracy\"]],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], \n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [True],\n",
    "        'epochs': [30, 50],\n",
    "        'batch':[32,64],\n",
    "        }\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter=20,  # Number of different combinations to sample\n",
    "            scoring = make_scorer(custom_accuracy), \n",
    "            cv=tscv,  # Number of cross-validation folds\n",
    "            verbose=2,  # Display search progress\n",
    "            n_jobs= 1,  # Use all available CPUs\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params_75.append(random_search.best_params_)\n",
    "        best_score_75.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 75 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 32, 'neurons': [128, 64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'relu', 'classification': True, 'batch': 32, 'activation': 'relu'}\n",
      "Best Scorefor 75 npast_timesteps: 0.9011414807833281\n"
     ]
    }
   ],
   "source": [
    "# take the best score and find associated parameters\n",
    "\n",
    "max_index_75 = best_score_75.index(max(best_score_75))\n",
    "print(f'Best Params for 75 npast_timesteps: {best_params_75[max_index_75]}')\n",
    "print(f'Best Scorefor 75 npast_timesteps: {max(best_score_75)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores_.append(max(best_score_75))\n",
    "best_params_.append(best_params_75[max_index_75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index_all = best_scores_.index(max(best_scores_))\n",
    "best_params_all = best_params_[max_index_all]\n",
    "\n",
    "print(f'Best Params for Classification: {best_params_all}')\n",
    "print(f'Best Score for Classification: {max(best_scores_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add manually to list becuase kernal crashed, this was test run with 10 iterations, cv = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9011414807833281\n",
      "2\n",
      "{'pool_size': 2, 'optimizer': 'adam', 'nfilters': 32, 'neurons': [128, 64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'relu', 'classification': True, 'batch': 32, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "scores = [0.8999931029726188, 0.8993370127390746,  0.9011414807833281]\n",
    "params = [{'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 25, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.1, 'conv_layer': False, 'conv_act': 'relu', 'classification': True, 'batch': 64, 'activation': 'tanh'},\n",
    "          {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 32, 'neurons': [128, 64, 32], 'n_timesteps': 50, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.1, 'conv_layer': False, 'conv_act': 'tanh', 'classification': True, 'batch': 64, 'activation': 'relu'},\n",
    "          {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 32, 'neurons': [128, 64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 1, 'n_features': 7, 'metrics': ['accuracy'], 'loss': 'binary_crossentropy', 'epochs': 30, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'relu', 'classification': True, 'batch': 32, 'activation': 'relu'}\n",
    "\n",
    "\n",
    "]\n",
    "max_index = scores.index(max(scores))\n",
    "print(max(scores))\n",
    "print(max_index)\n",
    "print(params[max_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
