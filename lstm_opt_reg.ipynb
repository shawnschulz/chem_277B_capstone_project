{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This notebook is to optimize hyperparameters for LSTM Model using Randomized Search'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This notebook is to optimize hyperparameters for LSTM Model For Regression using Randomized Search\"\"\"\n",
    "\n",
    "# seperated by npast_timesteps to make preprocessing easier (tried doing it inside the wrapper and ran into many shape issues) \n",
    "# kernal crashed when I tried to do it all in one for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:04:45.441292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 22:04:45.442366: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-11 22:04:45.453380: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-11 22:04:45.472460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733983485.493920  460378 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733983485.500943  460378 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 22:04:45.539459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nrsim_lstm import NRSIM_LSTM as lstm\n",
    "import utils277b as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Set Parameters for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>pH</th>\n",
       "      <th>Hydrogen</th>\n",
       "      <th>Total Gas</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Radioactivity</th>\n",
       "      <th>Power</th>\n",
       "      <th>Reactor Safety</th>\n",
       "      <th>Injection of Air</th>\n",
       "      <th>Injection of Air Degree</th>\n",
       "      <th>Resin Overheat</th>\n",
       "      <th>Resin Overheat Degree</th>\n",
       "      <th>Fuel Element Failure</th>\n",
       "      <th>Fuel Element Failure Degree</th>\n",
       "      <th>Chemical Addition</th>\n",
       "      <th>Vent Gas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11.000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.998</td>\n",
       "      <td>50.112998</td>\n",
       "      <td>60.112998</td>\n",
       "      <td>500.732703</td>\n",
       "      <td>2104.971916</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.996</td>\n",
       "      <td>50.225941</td>\n",
       "      <td>60.225941</td>\n",
       "      <td>501.463398</td>\n",
       "      <td>2109.930204</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.994</td>\n",
       "      <td>50.338517</td>\n",
       "      <td>60.338517</td>\n",
       "      <td>502.190083</td>\n",
       "      <td>2114.861274</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.992</td>\n",
       "      <td>50.450414</td>\n",
       "      <td>60.450414</td>\n",
       "      <td>502.910764</td>\n",
       "      <td>2119.751611</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time      pH   Hydrogen  Total Gas  Temperature     Pressure  \\\n",
       "0     0  11.000  50.000000  60.000000   500.000000  2100.000000   \n",
       "1     1  10.998  50.112998  60.112998   500.732703  2104.971916   \n",
       "2     2  10.996  50.225941  60.225941   501.463398  2109.930204   \n",
       "3     3  10.994  50.338517  60.338517   502.190083  2114.861274   \n",
       "4     4  10.992  50.450414  60.450414   502.910764  2119.751611   \n",
       "\n",
       "   Radioactivity  Power  Reactor Safety  Injection of Air  \\\n",
       "0           10.0  100.0               0               NaN   \n",
       "1           10.0  100.0               0               NaN   \n",
       "2           10.0  100.0               0               NaN   \n",
       "3           10.0  100.0               0               NaN   \n",
       "4           10.0  100.0               0               NaN   \n",
       "\n",
       "   Injection of Air Degree Resin Overheat Resin Overheat Degree  \\\n",
       "0                      NaN            NaN                   NaN   \n",
       "1                      NaN            NaN                   NaN   \n",
       "2                      NaN            NaN                   NaN   \n",
       "3                      NaN            NaN                   NaN   \n",
       "4                      NaN            NaN                   NaN   \n",
       "\n",
       "  Fuel Element Failure Fuel Element Failure Degree  Chemical Addition  \\\n",
       "0                  NaN                         NaN              False   \n",
       "1                  NaN                         NaN              False   \n",
       "2                  NaN                         NaN              False   \n",
       "3                  NaN                         NaN              False   \n",
       "4                  NaN                         NaN              False   \n",
       "\n",
       "   Vent Gas  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in data\n",
    "data = pd.read_csv(\"test_file\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_460378/1598807462.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "numerical = ['Time', 'pH', 'Hydrogen', 'Total Gas', 'Temperature', 'Pressure','Radioactivity', 'Power']\n",
    "categorical = ['Reactor Safety', 'Injection of Air', 'Injection of Air Degree', \n",
    "               'Resin Overheat', 'Resin Overheat Degree', 'Fuel Element Failure', \n",
    "               'Fuel Element Failure Degree', 'Chemical Addition', 'Vent Gas']\n",
    "\n",
    "\n",
    "# In case there are NaN values\n",
    "data = data.fillna(False)\n",
    "data[categorical] = data[categorical].astype(int) # set categorical as integer\n",
    "\n",
    "# make sure no NaN values\n",
    "unique_values = data.apply(pd.Series.unique)\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training matrices\n",
    "X = np.array(data[numerical].drop(columns=['Time']))\n",
    "Y = np.array(data[categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "class PreprocessingWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_timesteps=1, n_features=7, neurons=[32, 32], activation='tanh', \n",
    "                 optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                 dropout=0.2, conv_layer=False, nfilters=64, conv_act='relu', pool_size=2, \n",
    "                 classification=True, n_predicted_timesteps = 1, epochs = 50, batch = 64, n_predicted_features = 7):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.dropout = dropout\n",
    "        self.conv_layer = conv_layer\n",
    "        self.nfilters = nfilters\n",
    "        self.conv_act = conv_act\n",
    "        self.pool_size = pool_size\n",
    "        self.classification = classification\n",
    "        self.n_predicted_timesteps = n_predicted_timesteps\n",
    "        self.epochs = epochs\n",
    "        self.batch  = batch\n",
    "        self.n_predicted_features = n_predicted_features\n",
    "        \n",
    "    def create_model(self):\n",
    "\n",
    "        model = lstm(neurons=self.neurons, \n",
    "                     activation_func = self.activation, \n",
    "                     nTimesteps = self.n_timesteps, \n",
    "                     nFeatures = self.n_features, \n",
    "                     npredTimesteps = 1, \n",
    "                     npredFeatures = self.n_predicted_features, \n",
    "                     model_optimizer = self.optimizer, \n",
    "                     model_loss = self.loss, \n",
    "                     model_metrics = self.metrics, \n",
    "                     dropout=self.dropout, \n",
    "                     conv_layer=self.conv_layer, \n",
    "                     nfilters=self.nfilters, \n",
    "                     cact =self.conv_act, \n",
    "                     cpool=2,\n",
    "                     classify=self.classification)\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.classification:\n",
    "            self.classes_ = np.unique(y)\n",
    "        # print(f'shape of Y: {y.shape}')\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X, y, nEpochs = 1, nBatches= 64, val_split = 0.2, verb = 2, shuf = False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        # print(f'shape of x (predict): {X.shape}')\n",
    "        y_pred = self.model.predict(X)\n",
    "        # print(f'shape of y_pred: {y_pred.shape}')\n",
    "        if self.classification:\n",
    "            y_pred = (y_pred >= 0.5).astype(float)\n",
    "        # print(y_pred[0][0])\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(y_true, y_pred):\n",
    "    # print(f\"Shape of y_true in custom scoring function: {y_true.shape}\")\n",
    "    # print(y_true[0][0])\n",
    "    # print(f\"Shape of y_pred in custom scoring function: {y_pred.shape}\")\n",
    "    # print(y_pred[0][0])\n",
    "\n",
    "    # # flatten\n",
    "    y_true_2d = y_true.reshape(-1, y_true.shape[2])  # Shape becomes (2148, 9)\n",
    "    y_pred_2d = y_pred.reshape(-1, y_pred.shape[2])  # Shape becomes (2148, 9)\n",
    "\n",
    "    y_true_2d = y_true_2d.astype(int)\n",
    "    y_pred_2d = y_pred_2d.astype(int)\n",
    "\n",
    "    # print(f'shape of true and pred after reshape: {y_true_2d.shape} and {y_pred_2d.shape}')\n",
    "    # print(f'type: {type(y_true_2d[0][0])}')\n",
    "    # print(f\"Data type of y_true: {y_true_2d.dtype}\")\n",
    "    # print(f\"Data type of y_pred: {y_pred_2d.dtype}\")\n",
    "\n",
    "    mse = -np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier  # Example model\n",
    "from scipy.stats import randint, uniform  # For defining distributions\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 npast_timesteps\n",
    "- Timesteps tried are defined outside of the RandomizedSearch to make preprocessing easier and avoid nan scoring values\n",
    "- Tried to run 25, 50, 75 timesteps using for loop and list, but kernal died or randomized search terminated due to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:40:25.763550: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 3s - 190ms/step - loss: 0.2038 - mae: 0.3315 - mse: 0.2038 - val_loss: 0.1355 - val_mae: 0.2569 - val_mse: 0.1355\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 103ms/step - loss: 0.1546 - mae: 0.2819 - mse: 0.1546 - val_loss: 0.0806 - val_mae: 0.2119 - val_mse: 0.0806\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "40/40 - 3s - 76ms/step - loss: 0.1415 - mae: 0.2792 - mse: 0.1415 - val_loss: 0.0841 - val_mae: 0.2236 - val_mse: 0.0841\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 203ms/step - loss: 0.1360 - mae: 0.2721 - mse: 0.1360 - val_loss: 0.0779 - val_mae: 0.2162 - val_mse: 0.0779\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 101ms/step - loss: 0.1248 - mae: 0.2626 - mse: 0.1248 - val_loss: 0.0604 - val_mae: 0.1824 - val_mse: 0.0604\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 3s - 75ms/step - loss: 0.0969 - mae: 0.2382 - mse: 0.0969 - val_loss: 0.0642 - val_mae: 0.1939 - val_mse: 0.0642\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 2s - 144ms/step - loss: 0.1629 - mae: 0.2940 - mse: 0.1629 - val_loss: 0.1100 - val_mae: 0.2173 - val_mse: 0.1100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "27/27 - 5s - 169ms/step - loss: 0.1289 - mae: 0.2543 - mse: 0.1289 - val_loss: 0.0655 - val_mae: 0.1902 - val_mse: 0.0655\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "40/40 - 6s - 145ms/step - loss: 0.1291 - mae: 0.2596 - mse: 0.1291 - val_loss: 0.0693 - val_mae: 0.1982 - val_mse: 0.0693\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "14/14 - 2s - 172ms/step - loss: 0.2281 - mae: 0.3511 - mse: 0.2281 - val_loss: 0.1539 - val_mae: 0.2722 - val_mse: 0.1539\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 3s - 110ms/step - loss: 0.1546 - mae: 0.2907 - mse: 0.1546 - val_loss: 0.0818 - val_mae: 0.2087 - val_mse: 0.0818\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 3s - 85ms/step - loss: 0.1240 - mae: 0.2656 - mse: 0.1240 - val_loss: 0.0779 - val_mae: 0.2077 - val_mse: 0.0779\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 217ms/step - loss: 0.1283 - mae: 0.2616 - mse: 0.1283 - val_loss: 0.0717 - val_mae: 0.2018 - val_mse: 0.0717\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 1s - 29ms/step - loss: 0.1139 - mae: 0.2531 - mse: 0.1139 - val_loss: 0.0544 - val_mae: 0.1705 - val_mse: 0.0544\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 4s - 98ms/step - loss: 0.0914 - mae: 0.2344 - mse: 0.0914 - val_loss: 0.0637 - val_mae: 0.1952 - val_mse: 0.0637\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "14/14 - 3s - 195ms/step - loss: 0.1241 - mae: 0.2634 - mse: 0.1241 - val_loss: 0.0753 - val_mae: 0.1998 - val_mse: 0.0753\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 3s - 120ms/step - loss: 0.1363 - mae: 0.2707 - mse: 0.1363 - val_loss: 0.0674 - val_mae: 0.1949 - val_mse: 0.0674\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 4s - 98ms/step - loss: 0.0971 - mae: 0.2307 - mse: 0.0971 - val_loss: 0.0667 - val_mae: 0.1918 - val_mse: 0.0667\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 2s - 177ms/step - loss: 0.1874 - mae: 0.3114 - mse: 0.1874 - val_loss: 0.1073 - val_mae: 0.2301 - val_mse: 0.1073\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 105ms/step - loss: 0.1451 - mae: 0.2817 - mse: 0.1451 - val_loss: 0.0833 - val_mae: 0.2092 - val_mse: 0.0833\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 3s - 76ms/step - loss: 0.1285 - mae: 0.2626 - mse: 0.1285 - val_loss: 0.0790 - val_mae: 0.2093 - val_mse: 0.0790\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 2s - 124ms/step - loss: 0.1267 - mae: 0.2697 - mse: 0.1267 - val_loss: 0.0630 - val_mae: 0.1852 - val_mse: 0.0630\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "27/27 - 6s - 207ms/step - loss: 0.0948 - mae: 0.2303 - mse: 0.0948 - val_loss: 0.0474 - val_mae: 0.1664 - val_mse: 0.0474\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "40/40 - 5s - 136ms/step - loss: 0.1034 - mae: 0.2423 - mse: 0.1034 - val_loss: 0.0571 - val_mae: 0.1743 - val_mse: 0.0571\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "14/14 - 2s - 174ms/step - loss: 0.1542 - mae: 0.2909 - mse: 0.1542 - val_loss: 0.0773 - val_mae: 0.2057 - val_mse: 0.0773\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "27/27 - 3s - 109ms/step - loss: 0.1081 - mae: 0.2528 - mse: 0.1081 - val_loss: 0.0607 - val_mae: 0.1899 - val_mse: 0.0607\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 3s - 80ms/step - loss: 0.1037 - mae: 0.2393 - mse: 0.1037 - val_loss: 0.0674 - val_mae: 0.1948 - val_mse: 0.0674\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "14/14 - 3s - 185ms/step - loss: 0.1386 - mae: 0.2688 - mse: 0.1386 - val_loss: 0.0781 - val_mae: 0.1987 - val_mse: 0.0781\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 1s - 31ms/step - loss: 0.1255 - mae: 0.2709 - mse: 0.1255 - val_loss: 0.0641 - val_mae: 0.1897 - val_mse: 0.0641\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "40/40 - 3s - 80ms/step - loss: 0.1042 - mae: 0.2400 - mse: 0.1042 - val_loss: 0.0689 - val_mae: 0.1974 - val_mse: 0.0689\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 180ms/step - loss: 0.1581 - mae: 0.3012 - mse: 0.1581 - val_loss: 0.0753 - val_mae: 0.2052 - val_mse: 0.0753\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 113ms/step - loss: 0.1030 - mae: 0.2421 - mse: 0.1030 - val_loss: 0.0564 - val_mae: 0.1763 - val_mse: 0.0564\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "40/40 - 4s - 102ms/step - loss: 0.0949 - mae: 0.2282 - mse: 0.0949 - val_loss: 0.0658 - val_mae: 0.1918 - val_mse: 0.0658\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "14/14 - 3s - 196ms/step - loss: 0.1939 - mae: 0.3264 - mse: 0.1939 - val_loss: 0.1315 - val_mae: 0.2614 - val_mse: 0.1315\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "27/27 - 3s - 114ms/step - loss: 0.1555 - mae: 0.2990 - mse: 0.1555 - val_loss: 0.0880 - val_mae: 0.2241 - val_mse: 0.0880\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 4s - 107ms/step - loss: 0.1298 - mae: 0.2596 - mse: 0.1298 - val_loss: 0.0838 - val_mae: 0.2209 - val_mse: 0.0838\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "14/14 - 2s - 170ms/step - loss: 0.1939 - mae: 0.3182 - mse: 0.1939 - val_loss: 0.1044 - val_mae: 0.2535 - val_mse: 0.1044\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "27/27 - 5s - 195ms/step - loss: 0.1277 - mae: 0.2582 - mse: 0.1277 - val_loss: 0.0576 - val_mae: 0.1848 - val_mse: 0.0576\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "40/40 - 6s - 159ms/step - loss: 0.1360 - mae: 0.2744 - mse: 0.1360 - val_loss: 0.0644 - val_mae: 0.1888 - val_mse: 0.0644\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "14/14 - 3s - 202ms/step - loss: 0.1956 - mae: 0.3219 - mse: 0.1956 - val_loss: 0.1273 - val_mae: 0.2546 - val_mse: 0.1273\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "27/27 - 3s - 111ms/step - loss: 0.1653 - mae: 0.2997 - mse: 0.1653 - val_loss: 0.0866 - val_mae: 0.2106 - val_mse: 0.0866\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "40/40 - 4s - 93ms/step - loss: 0.1442 - mae: 0.2756 - mse: 0.1442 - val_loss: 0.0825 - val_mae: 0.2117 - val_mse: 0.0825\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "14/14 - 4s - 254ms/step - loss: 0.1806 - mae: 0.3142 - mse: 0.1806 - val_loss: 0.0899 - val_mae: 0.2089 - val_mse: 0.0899\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "27/27 - 5s - 198ms/step - loss: 0.1327 - mae: 0.2754 - mse: 0.1327 - val_loss: 0.0475 - val_mae: 0.1636 - val_mse: 0.0475\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "40/40 - 7s - 182ms/step - loss: 0.1515 - mae: 0.2869 - mse: 0.1515 - val_loss: 0.0817 - val_mae: 0.2164 - val_mse: 0.0817\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "14/14 - 5s - 370ms/step - loss: 0.1263 - mae: 0.2716 - mse: 0.1263 - val_loss: 0.0628 - val_mae: 0.1939 - val_mse: 0.0628\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "27/27 - 4s - 142ms/step - loss: 0.0965 - mae: 0.2349 - mse: 0.0965 - val_loss: 0.0521 - val_mae: 0.1708 - val_mse: 0.0521\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "40/40 - 7s - 169ms/step - loss: 0.0777 - mae: 0.2118 - mse: 0.0777 - val_loss: 0.0617 - val_mae: 0.1812 - val_mse: 0.0617\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "14/14 - 3s - 242ms/step - loss: 0.1379 - mae: 0.2839 - mse: 0.1379 - val_loss: 0.0733 - val_mae: 0.1980 - val_mse: 0.0733\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "27/27 - 4s - 156ms/step - loss: 0.1104 - mae: 0.2521 - mse: 0.1104 - val_loss: 0.0575 - val_mae: 0.1821 - val_mse: 0.0575\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "40/40 - 5s - 137ms/step - loss: 0.1064 - mae: 0.2479 - mse: 0.1064 - val_loss: 0.0702 - val_mae: 0.1990 - val_mse: 0.0702\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n",
      "14/14 - 1s - 82ms/step - loss: 0.1329 - mae: 0.2751 - mse: 0.1329 - val_loss: 0.0782 - val_mae: 0.2074 - val_mse: 0.0782\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "27/27 - 6s - 206ms/step - loss: 0.1081 - mae: 0.2476 - mse: 0.1081 - val_loss: 0.0566 - val_mae: 0.1777 - val_mse: 0.0566\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
      "40/40 - 7s - 172ms/step - loss: 0.0996 - mae: 0.2377 - mse: 0.0996 - val_loss: 0.0662 - val_mae: 0.1907 - val_mse: 0.0662\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "14/14 - 6s - 430ms/step - loss: 0.1905 - mae: 0.3203 - mse: 0.1905 - val_loss: 0.1178 - val_mae: 0.2495 - val_mse: 0.1178\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "27/27 - 6s - 221ms/step - loss: 0.1354 - mae: 0.2649 - mse: 0.1354 - val_loss: 0.0688 - val_mae: 0.1985 - val_mse: 0.0688\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m 28ms/step\n",
      "40/40 - 8s - 197ms/step - loss: 0.1201 - mae: 0.2554 - mse: 0.1201 - val_loss: 0.0749 - val_mae: 0.2021 - val_mse: 0.0749\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step\n",
      "14/14 - 4s - 298ms/step - loss: 0.2078 - mae: 0.3359 - mse: 0.2078 - val_loss: 0.1400 - val_mae: 0.2611 - val_mse: 0.1400\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "27/27 - 5s - 168ms/step - loss: 0.1213 - mae: 0.2631 - mse: 0.1213 - val_loss: 0.0715 - val_mae: 0.2049 - val_mse: 0.0715\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "40/40 - 5s - 113ms/step - loss: 0.1368 - mae: 0.2698 - mse: 0.1368 - val_loss: 0.0761 - val_mae: 0.2168 - val_mse: 0.0761\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 7s - 126ms/step - loss: 0.0836 - mae: 0.2203 - mse: 0.0836 - val_loss: 0.0850 - val_mae: 0.2350 - val_mse: 0.0850\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "14/14 - 5s - 372ms/step - loss: 0.2021 - mae: 0.3199 - mse: 0.2021 - val_loss: 0.1186 - val_mae: 0.2491 - val_mse: 0.1186\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "27/27 - 4s - 136ms/step - loss: 0.1406 - mae: 0.2764 - mse: 0.1406 - val_loss: 0.0806 - val_mae: 0.2057 - val_mse: 0.0806\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 4s - 94ms/step - loss: 0.1377 - mae: 0.2663 - mse: 0.1377 - val_loss: 0.0858 - val_mae: 0.2174 - val_mse: 0.0858\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 4s - 274ms/step - loss: 0.1899 - mae: 0.3206 - mse: 0.1899 - val_loss: 0.1034 - val_mae: 0.2390 - val_mse: 0.1034\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "27/27 - 4s - 131ms/step - loss: 0.1496 - mae: 0.2762 - mse: 0.1496 - val_loss: 0.0607 - val_mae: 0.1790 - val_mse: 0.0607\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 1s - 31ms/step - loss: 0.1134 - mae: 0.2407 - mse: 0.1134 - val_loss: 0.0696 - val_mae: 0.1999 - val_mse: 0.0696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 5s - 322ms/step - loss: 0.2250 - mae: 0.3425 - mse: 0.2250 - val_loss: 0.1631 - val_mae: 0.2817 - val_mse: 0.1631\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "27/27 - 4s - 164ms/step - loss: 0.1860 - mae: 0.3159 - mse: 0.1860 - val_loss: 0.1190 - val_mae: 0.2707 - val_mse: 0.1190\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "40/40 - 5s - 115ms/step - loss: 0.1597 - mae: 0.2844 - mse: 0.1597 - val_loss: 0.1094 - val_mae: 0.2440 - val_mse: 0.1094\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "14/14 - 3s - 227ms/step - loss: 0.2122 - mae: 0.3322 - mse: 0.2122 - val_loss: 0.1323 - val_mae: 0.2575 - val_mse: 0.1323\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 3s - 115ms/step - loss: 0.1775 - mae: 0.3037 - mse: 0.1775 - val_loss: 0.0938 - val_mae: 0.2195 - val_mse: 0.0938\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 5s - 133ms/step - loss: 0.1512 - mae: 0.2821 - mse: 0.1512 - val_loss: 0.0942 - val_mae: 0.2273 - val_mse: 0.0942\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m -52483us/step\n",
      "14/14 - 3s - 221ms/step - loss: 0.1700 - mae: 0.2962 - mse: 0.1700 - val_loss: 0.0789 - val_mae: 0.2022 - val_mse: 0.0789\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "27/27 - 3s - 117ms/step - loss: 0.1204 - mae: 0.2527 - mse: 0.1204 - val_loss: 0.0598 - val_mae: 0.1804 - val_mse: 0.0598\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 4s - 88ms/step - loss: 0.1050 - mae: 0.2367 - mse: 0.1050 - val_loss: 0.0658 - val_mae: 0.1928 - val_mse: 0.0658\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "14/14 - 3s - 223ms/step - loss: 0.1956 - mae: 0.3179 - mse: 0.1956 - val_loss: 0.1126 - val_mae: 0.2285 - val_mse: 0.1126\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 3s - 124ms/step - loss: 0.1490 - mae: 0.2750 - mse: 0.1490 - val_loss: 0.0720 - val_mae: 0.2071 - val_mse: 0.0720\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 3s - 84ms/step - loss: 0.1285 - mae: 0.2546 - mse: 0.1285 - val_loss: 0.0741 - val_mae: 0.2019 - val_mse: 0.0741\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 207ms/step - loss: 0.2160 - mae: 0.3394 - mse: 0.2160 - val_loss: 0.1425 - val_mae: 0.2649 - val_mse: 0.1425\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 3s - 122ms/step - loss: 0.1692 - mae: 0.3022 - mse: 0.1692 - val_loss: 0.0911 - val_mae: 0.2207 - val_mse: 0.0911\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m 12ms/step    \n",
      "40/40 - 3s - 78ms/step - loss: 0.1603 - mae: 0.2923 - mse: 0.1603 - val_loss: 0.1051 - val_mae: 0.2360 - val_mse: 0.1051\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 5s - 345ms/step - loss: 0.1837 - mae: 0.3079 - mse: 0.1837 - val_loss: 0.0800 - val_mae: 0.1841 - val_mse: 0.0800\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "27/27 - 5s - 176ms/step - loss: 0.1324 - mae: 0.2648 - mse: 0.1324 - val_loss: 0.0552 - val_mae: 0.1734 - val_mse: 0.0552\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "40/40 - 5s - 116ms/step - loss: 0.1102 - mae: 0.2384 - mse: 0.1102 - val_loss: 0.0662 - val_mae: 0.1971 - val_mse: 0.0662\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "14/14 - 3s - 234ms/step - loss: 0.1860 - mae: 0.3099 - mse: 0.1860 - val_loss: 0.1039 - val_mae: 0.2060 - val_mse: 0.1039\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 4s - 154ms/step - loss: 0.1401 - mae: 0.2724 - mse: 0.1401 - val_loss: 0.0637 - val_mae: 0.1953 - val_mse: 0.0637\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 4s - 96ms/step - loss: 0.1156 - mae: 0.2490 - mse: 0.1156 - val_loss: 0.0722 - val_mae: 0.2034 - val_mse: 0.0722\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "14/14 - 4s - 251ms/step - loss: 0.1995 - mae: 0.3260 - mse: 0.1995 - val_loss: 0.1206 - val_mae: 0.2462 - val_mse: 0.1206\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 5s - 167ms/step - loss: 0.1612 - mae: 0.2820 - mse: 0.1612 - val_loss: 0.0705 - val_mae: 0.1899 - val_mse: 0.0705\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "40/40 - 4s - 100ms/step - loss: 0.1294 - mae: 0.2551 - mse: 0.1294 - val_loss: 0.0722 - val_mae: 0.2019 - val_mse: 0.0722\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 225ms/step - loss: 0.1335 - mae: 0.2646 - mse: 0.1335 - val_loss: 0.0802 - val_mae: 0.2153 - val_mse: 0.0802\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 119ms/step - loss: 0.1119 - mae: 0.2457 - mse: 0.1119 - val_loss: 0.0538 - val_mae: 0.1742 - val_mse: 0.0538\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 1s - 32ms/step - loss: 0.0896 - mae: 0.2219 - mse: 0.0896 - val_loss: 0.0647 - val_mae: 0.1963 - val_mse: 0.0647\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 3s - 218ms/step - loss: 0.2153 - mae: 0.3358 - mse: 0.2153 - val_loss: 0.1504 - val_mae: 0.2752 - val_mse: 0.1504\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "27/27 - 4s - 152ms/step - loss: 0.1799 - mae: 0.3000 - mse: 0.1799 - val_loss: 0.1025 - val_mae: 0.2302 - val_mse: 0.1025\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 3s - 85ms/step - loss: 0.1557 - mae: 0.2830 - mse: 0.1557 - val_loss: 0.1095 - val_mae: 0.2548 - val_mse: 0.1095\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "14/14 - 4s - 304ms/step - loss: 0.2257 - mae: 0.3430 - mse: 0.2257 - val_loss: 0.1656 - val_mae: 0.2831 - val_mse: 0.1656\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "27/27 - 4s - 153ms/step - loss: 0.1782 - mae: 0.3002 - mse: 0.1782 - val_loss: 0.0991 - val_mae: 0.2266 - val_mse: 0.0991\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "40/40 - 4s - 110ms/step - loss: 0.1564 - mae: 0.2809 - mse: 0.1564 - val_loss: 0.1030 - val_mae: 0.2413 - val_mse: 0.1030\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "14/14 - 1s - 84ms/step - loss: 0.2156 - mae: 0.3372 - mse: 0.2156 - val_loss: 0.1370 - val_mae: 0.2528 - val_mse: 0.1370\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "27/27 - 8s - 284ms/step - loss: 0.1632 - mae: 0.2941 - mse: 0.1632 - val_loss: 0.0910 - val_mae: 0.2131 - val_mse: 0.0910\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "40/40 - 4s - 94ms/step - loss: 0.1607 - mae: 0.2973 - mse: 0.1607 - val_loss: 0.0882 - val_mae: 0.2215 - val_mse: 0.0882\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 5s - 348ms/step - loss: 0.2221 - mae: 0.3411 - mse: 0.2221 - val_loss: 0.1549 - val_mae: 0.2764 - val_mse: 0.1549\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "27/27 - 5s - 200ms/step - loss: 0.1811 - mae: 0.3081 - mse: 0.1811 - val_loss: 0.1008 - val_mae: 0.2415 - val_mse: 0.1008\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "40/40 - 3s - 78ms/step - loss: 0.1667 - mae: 0.2932 - mse: 0.1667 - val_loss: 0.1073 - val_mae: 0.2409 - val_mse: 0.1073\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "14/14 - 4s - 304ms/step - loss: 0.1840 - mae: 0.3075 - mse: 0.1840 - val_loss: 0.0866 - val_mae: 0.2073 - val_mse: 0.0866\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "27/27 - 5s - 185ms/step - loss: 0.1428 - mae: 0.2731 - mse: 0.1428 - val_loss: 0.0544 - val_mae: 0.1706 - val_mse: 0.0544\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "40/40 - 6s - 156ms/step - loss: 0.1096 - mae: 0.2374 - mse: 0.1096 - val_loss: 0.0653 - val_mae: 0.1903 - val_mse: 0.0653\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "14/14 - 4s - 273ms/step - loss: 0.1683 - mae: 0.2976 - mse: 0.1683 - val_loss: 0.0797 - val_mae: 0.1990 - val_mse: 0.0797\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 2s - 60ms/step - loss: 0.1322 - mae: 0.2698 - mse: 0.1322 - val_loss: 0.0593 - val_mae: 0.1832 - val_mse: 0.0593\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 3s - 85ms/step - loss: 0.1064 - mae: 0.2406 - mse: 0.1064 - val_loss: 0.0681 - val_mae: 0.1985 - val_mse: 0.0681\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 3s - 227ms/step - loss: 0.1763 - mae: 0.3058 - mse: 0.1763 - val_loss: 0.0935 - val_mae: 0.2138 - val_mse: 0.0935\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "27/27 - 3s - 122ms/step - loss: 0.1187 - mae: 0.2566 - mse: 0.1187 - val_loss: 0.0522 - val_mae: 0.1685 - val_mse: 0.0522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "40/40 - 3s - 87ms/step - loss: 0.0965 - mae: 0.2336 - mse: 0.0965 - val_loss: 0.0633 - val_mae: 0.1894 - val_mse: 0.0633\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "14/14 - 5s - 332ms/step - loss: 0.2175 - mae: 0.3368 - mse: 0.2175 - val_loss: 0.1336 - val_mae: 0.2481 - val_mse: 0.1336\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "27/27 - 6s - 208ms/step - loss: 0.1761 - mae: 0.3031 - mse: 0.1761 - val_loss: 0.0929 - val_mae: 0.2150 - val_mse: 0.0929\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "40/40 - 4s - 89ms/step - loss: 0.1394 - mae: 0.2702 - mse: 0.1394 - val_loss: 0.0880 - val_mae: 0.2246 - val_mse: 0.0880\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "14/14 - 4s - 309ms/step - loss: 0.2235 - mae: 0.3445 - mse: 0.2235 - val_loss: 0.1596 - val_mae: 0.2859 - val_mse: 0.1596\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "27/27 - 4s - 134ms/step - loss: 0.1869 - mae: 0.3149 - mse: 0.1869 - val_loss: 0.1117 - val_mae: 0.2399 - val_mse: 0.1117\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "40/40 - 4s - 98ms/step - loss: 0.1580 - mae: 0.2853 - mse: 0.1580 - val_loss: 0.1122 - val_mae: 0.2525 - val_mse: 0.1122\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "53/53 - 4s - 73ms/step - loss: 0.0958 - mae: 0.2266 - mse: 0.0958 - val_loss: 0.0832 - val_mae: 0.2297 - val_mse: 0.0832\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [25]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params = []\n",
    "best_score = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[numerical].drop(columns=['Time']))\n",
    "\n",
    "    # preprocess data (2D)\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 60) \n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 60) \n",
    "\n",
    "    # print(f'shape of y: {Y.shape}')\n",
    "    for conv in conv_layer: \n",
    "        # will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\n",
    "        \n",
    "        if conv: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [60],\n",
    "        'n_predicted_features' : [7],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"mse\"],\n",
    "        'metrics': [[\"mse\", 'mae']],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], # 32, 64\n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [False],\n",
    "        'epochs':[10, 50], # 50\n",
    "        'batch':[64],\n",
    "        }\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter=20,  # Number of different combinations to sample\n",
    "            scoring = make_scorer(custom_mse), \n",
    "            cv=tscv,  # Number of cross-validation folds\n",
    "            verbose=1,  # Display search progress\n",
    "            n_jobs= 1,  \n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params.append(random_search.best_params_)\n",
    "        best_score.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 25 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [128, 64, 32], 'n_timesteps': 25, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
      "Best Score for 25 npast_timesteps: -0.07911169500806445\n"
     ]
    }
   ],
   "source": [
    "# # take the best score and find associated parameters\n",
    "\n",
    "max_index = best_score.index(max(best_score))\n",
    "print(f'Best Params for 25 npast_timesteps: {best_params[max_index]}')\n",
    "print(f'Best Score for 25 npast_timesteps: {max(best_score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Params for 25 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [128, 64, 32], 'n_timesteps': 25, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
    "Best Score for 25 npast_timesteps: -0.07911169500806445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to list\n",
    "best_scores_ = []\n",
    "best_params_ = []\n",
    "\n",
    "best_scores_.append(max(best_score))\n",
    "best_params_.append(best_params[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 npast_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:51:24.810678: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 3s - 215ms/step - loss: 0.1984 - mae: 0.3259 - mse: 0.1984 - val_loss: 0.1310 - val_mae: 0.2700 - val_mse: 0.1310\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "27/27 - 4s - 131ms/step - loss: 0.1348 - mae: 0.2643 - mse: 0.1348 - val_loss: 0.0678 - val_mae: 0.1929 - val_mse: 0.0678\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "40/40 - 6s - 151ms/step - loss: 0.1105 - mae: 0.2417 - mse: 0.1105 - val_loss: 0.0883 - val_mae: 0.2242 - val_mse: 0.0883\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.6s\n",
      "14/14 - 4s - 284ms/step - loss: 0.1375 - mae: 0.2848 - mse: 0.1375 - val_loss: 0.0773 - val_mae: 0.2154 - val_mse: 0.0773\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "27/27 - 4s - 165ms/step - loss: 0.0923 - mae: 0.2270 - mse: 0.0923 - val_loss: 0.0466 - val_mae: 0.1661 - val_mse: 0.0466\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m -41178us/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "40/40 - 6s - 141ms/step - loss: 0.1075 - mae: 0.2414 - mse: 0.1075 - val_loss: 0.0649 - val_mae: 0.1912 - val_mse: 0.0649\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "14/14 - 6s - 452ms/step - loss: 0.1729 - mae: 0.3061 - mse: 0.1729 - val_loss: 0.1156 - val_mae: 0.2243 - val_mse: 0.1156\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.3s\n",
      "27/27 - 8s - 296ms/step - loss: 0.1480 - mae: 0.2776 - mse: 0.1480 - val_loss: 0.0791 - val_mae: 0.2113 - val_mse: 0.0791\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.8s\n",
      "40/40 - 8s - 201ms/step - loss: 0.1308 - mae: 0.2628 - mse: 0.1308 - val_loss: 0.0672 - val_mae: 0.1914 - val_mse: 0.0672\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.0s\n",
      "14/14 - 4s - 257ms/step - loss: 0.2098 - mae: 0.3424 - mse: 0.2098 - val_loss: 0.1448 - val_mae: 0.2646 - val_mse: 0.1448\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "27/27 - 5s - 180ms/step - loss: 0.1451 - mae: 0.2794 - mse: 0.1451 - val_loss: 0.0691 - val_mae: 0.1910 - val_mse: 0.0691\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "40/40 - 6s - 150ms/step - loss: 0.1340 - mae: 0.2736 - mse: 0.1340 - val_loss: 0.0954 - val_mae: 0.2395 - val_mse: 0.0954\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "14/14 - 4s - 320ms/step - loss: 0.1547 - mae: 0.2904 - mse: 0.1547 - val_loss: 0.0772 - val_mae: 0.2017 - val_mse: 0.0772\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "27/27 - 4s - 159ms/step - loss: 0.1267 - mae: 0.2689 - mse: 0.1267 - val_loss: 0.0564 - val_mae: 0.1732 - val_mse: 0.0564\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "40/40 - 7s - 182ms/step - loss: 0.0829 - mae: 0.2159 - mse: 0.0829 - val_loss: 0.0586 - val_mae: 0.1790 - val_mse: 0.0586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "14/14 - 4s - 254ms/step - loss: 0.1754 - mae: 0.3132 - mse: 0.1754 - val_loss: 0.0920 - val_mae: 0.2151 - val_mse: 0.0920\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "27/27 - 4s - 143ms/step - loss: 0.1361 - mae: 0.2731 - mse: 0.1361 - val_loss: 0.0570 - val_mae: 0.1779 - val_mse: 0.0570\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "40/40 - 6s - 148ms/step - loss: 0.1017 - mae: 0.2422 - mse: 0.1017 - val_loss: 0.0741 - val_mae: 0.2110 - val_mse: 0.0741\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "14/14 - 1s - 75ms/step - loss: 0.1594 - mae: 0.2979 - mse: 0.1594 - val_loss: 0.1052 - val_mae: 0.2300 - val_mse: 0.1052\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   2.1s\n",
      "27/27 - 4s - 163ms/step - loss: 0.1497 - mae: 0.2887 - mse: 0.1497 - val_loss: 0.0818 - val_mae: 0.2111 - val_mse: 0.0818\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "40/40 - 5s - 130ms/step - loss: 0.1445 - mae: 0.2737 - mse: 0.1445 - val_loss: 0.0849 - val_mae: 0.2251 - val_mse: 0.0849\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "14/14 - 6s - 394ms/step - loss: 0.1219 - mae: 0.2646 - mse: 0.1219 - val_loss: 0.0762 - val_mae: 0.2063 - val_mse: 0.0762\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.4s\n",
      "27/27 - 9s - 337ms/step - loss: 0.1078 - mae: 0.2492 - mse: 0.1078 - val_loss: 0.0517 - val_mae: 0.1858 - val_mse: 0.0517\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -18784us/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   9.3s\n",
      "40/40 - 11s - 267ms/step - loss: 0.0867 - mae: 0.2205 - mse: 0.0867 - val_loss: 0.0624 - val_mae: 0.1874 - val_mse: 0.0624\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.9s\n",
      "14/14 - 3s - 242ms/step - loss: 0.1742 - mae: 0.3162 - mse: 0.1742 - val_loss: 0.0840 - val_mae: 0.2213 - val_mse: 0.0840\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "27/27 - 4s - 146ms/step - loss: 0.1250 - mae: 0.2669 - mse: 0.1250 - val_loss: 0.0564 - val_mae: 0.1751 - val_mse: 0.0564\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "40/40 - 6s - 138ms/step - loss: 0.0907 - mae: 0.2276 - mse: 0.0907 - val_loss: 0.0719 - val_mae: 0.2020 - val_mse: 0.0719\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 2s - 130ms/step - loss: 0.1713 - mae: 0.3145 - mse: 0.1713 - val_loss: 0.0735 - val_mae: 0.1943 - val_mse: 0.0735\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 5s - 190ms/step - loss: 0.0932 - mae: 0.2335 - mse: 0.0932 - val_loss: 0.0538 - val_mae: 0.1676 - val_mse: 0.0538\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.4s\n",
      "40/40 - 6s - 157ms/step - loss: 0.0894 - mae: 0.2224 - mse: 0.0894 - val_loss: 0.0637 - val_mae: 0.1850 - val_mse: 0.0637\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.0s\n",
      "14/14 - 4s - 316ms/step - loss: 0.1353 - mae: 0.2822 - mse: 0.1353 - val_loss: 0.0699 - val_mae: 0.2066 - val_mse: 0.0699\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "27/27 - 5s - 188ms/step - loss: 0.1019 - mae: 0.2427 - mse: 0.1019 - val_loss: 0.0549 - val_mae: 0.1812 - val_mse: 0.0549\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "40/40 - 4s - 102ms/step - loss: 0.0928 - mae: 0.2291 - mse: 0.0928 - val_loss: 0.0586 - val_mae: 0.1837 - val_mse: 0.0586\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "14/14 - 5s - 326ms/step - loss: 0.1997 - mae: 0.3209 - mse: 0.1997 - val_loss: 0.1320 - val_mae: 0.2433 - val_mse: 0.1320\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 5s - 172ms/step - loss: 0.1550 - mae: 0.2893 - mse: 0.1550 - val_loss: 0.0789 - val_mae: 0.2103 - val_mse: 0.0789\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "40/40 - 6s - 149ms/step - loss: 0.1529 - mae: 0.2786 - mse: 0.1529 - val_loss: 0.0950 - val_mae: 0.2295 - val_mse: 0.0950\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "14/14 - 6s - 427ms/step - loss: 0.1861 - mae: 0.3135 - mse: 0.1861 - val_loss: 0.1226 - val_mae: 0.2437 - val_mse: 0.1226\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -20505us/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 9s - 318ms/step - loss: 0.1661 - mae: 0.3058 - mse: 0.1661 - val_loss: 0.0833 - val_mae: 0.2142 - val_mse: 0.0833\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.8s\n",
      "40/40 - 11s - 282ms/step - loss: 0.1093 - mae: 0.2472 - mse: 0.1093 - val_loss: 0.0709 - val_mae: 0.2109 - val_mse: 0.0709\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  14.7s\n",
      "14/14 - 4s - 290ms/step - loss: 0.1850 - mae: 0.3154 - mse: 0.1850 - val_loss: 0.1223 - val_mae: 0.2384 - val_mse: 0.1223\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 3s - 114ms/step - loss: 0.1436 - mae: 0.2808 - mse: 0.1436 - val_loss: 0.0841 - val_mae: 0.2087 - val_mse: 0.0841\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "40/40 - 6s - 157ms/step - loss: 0.1198 - mae: 0.2491 - mse: 0.1198 - val_loss: 0.0705 - val_mae: 0.2039 - val_mse: 0.0705\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.6s\n",
      "14/14 - 8s - 570ms/step - loss: 0.1771 - mae: 0.3121 - mse: 0.1771 - val_loss: 0.1172 - val_mae: 0.2414 - val_mse: 0.1172\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.8s\n",
      "27/27 - 7s - 247ms/step - loss: 0.1483 - mae: 0.2834 - mse: 0.1483 - val_loss: 0.0659 - val_mae: 0.1842 - val_mse: 0.0659\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "40/40 - 10s - 259ms/step - loss: 0.1232 - mae: 0.2587 - mse: 0.1232 - val_loss: 0.0856 - val_mae: 0.2014 - val_mse: 0.0856\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.3s\n",
      "14/14 - 6s - 423ms/step - loss: 0.1270 - mae: 0.2657 - mse: 0.1270 - val_loss: 0.0683 - val_mae: 0.1891 - val_mse: 0.0683\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.2s\n",
      "27/27 - 8s - 310ms/step - loss: 0.1013 - mae: 0.2416 - mse: 0.1013 - val_loss: 0.0463 - val_mae: 0.1644 - val_mse: 0.0463\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "40/40 - 10s - 253ms/step - loss: 0.0917 - mae: 0.2307 - mse: 0.0917 - val_loss: 0.0660 - val_mae: 0.1924 - val_mse: 0.0660\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.3s\n",
      "14/14 - 4s - 266ms/step - loss: 0.1198 - mae: 0.2596 - mse: 0.1198 - val_loss: 0.0708 - val_mae: 0.2035 - val_mse: 0.0708\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 5s - 185ms/step - loss: 0.1055 - mae: 0.2448 - mse: 0.1055 - val_loss: 0.0550 - val_mae: 0.1703 - val_mse: 0.0550\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "40/40 - 6s - 143ms/step - loss: 0.1061 - mae: 0.2431 - mse: 0.1061 - val_loss: 0.0628 - val_mae: 0.1844 - val_mse: 0.0628\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "14/14 - 4s - 300ms/step - loss: 0.1538 - mae: 0.2962 - mse: 0.1538 - val_loss: 0.0826 - val_mae: 0.2153 - val_mse: 0.0826\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "27/27 - 6s - 212ms/step - loss: 0.1137 - mae: 0.2580 - mse: 0.1137 - val_loss: 0.0543 - val_mae: 0.1775 - val_mse: 0.0543\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.1s\n",
      "40/40 - 7s - 171ms/step - loss: 0.1047 - mae: 0.2435 - mse: 0.1047 - val_loss: 0.0612 - val_mae: 0.1841 - val_mse: 0.0612\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.4s\n",
      "14/14 - 6s - 442ms/step - loss: 0.1876 - mae: 0.3205 - mse: 0.1876 - val_loss: 0.1130 - val_mae: 0.2435 - val_mse: 0.1130\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.5s\n",
      "27/27 - 8s - 300ms/step - loss: 0.1197 - mae: 0.2557 - mse: 0.1197 - val_loss: 0.0595 - val_mae: 0.1823 - val_mse: 0.0595\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.5s\n",
      "40/40 - 10s - 261ms/step - loss: 0.1123 - mae: 0.2422 - mse: 0.1123 - val_loss: 0.0642 - val_mae: 0.1836 - val_mse: 0.0642\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.4s\n",
      "14/14 - 4s - 258ms/step - loss: 0.2104 - mae: 0.3432 - mse: 0.2104 - val_loss: 0.1510 - val_mae: 0.2835 - val_mse: 0.1510\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 4s - 158ms/step - loss: 0.1659 - mae: 0.3013 - mse: 0.1659 - val_loss: 0.0975 - val_mae: 0.2442 - val_mse: 0.0975\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.2s\n",
      "40/40 - 7s - 163ms/step - loss: 0.1441 - mae: 0.2739 - mse: 0.1441 - val_loss: 0.0946 - val_mae: 0.2328 - val_mse: 0.0946\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 8s - 152ms/step - loss: 0.0781 - mae: 0.2098 - mse: 0.0781 - val_loss: 0.0764 - val_mae: 0.2310 - val_mse: 0.0764\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "14/14 - 7s - 473ms/step - loss: 0.1645 - mae: 0.2959 - mse: 0.1645 - val_loss: 0.1115 - val_mae: 0.2328 - val_mse: 0.1115\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "27/27 - 4s - 142ms/step - loss: 0.1445 - mae: 0.2808 - mse: 0.1445 - val_loss: 0.0744 - val_mae: 0.1986 - val_mse: 0.0744\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "40/40 - 2s - 52ms/step - loss: 0.1273 - mae: 0.2669 - mse: 0.1273 - val_loss: 0.0828 - val_mae: 0.2249 - val_mse: 0.0828\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.1s\n",
      "14/14 - 3s - 240ms/step - loss: 0.1552 - mae: 0.2846 - mse: 0.1552 - val_loss: 0.0706 - val_mae: 0.1904 - val_mse: 0.0706\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 3s - 120ms/step - loss: 0.1174 - mae: 0.2447 - mse: 0.1174 - val_loss: 0.0558 - val_mae: 0.1757 - val_mse: 0.0558\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 3s - 82ms/step - loss: 0.1054 - mae: 0.2350 - mse: 0.1054 - val_loss: 0.0636 - val_mae: 0.1881 - val_mse: 0.0636\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "14/14 - 5s - 353ms/step - loss: 0.2206 - mae: 0.3385 - mse: 0.2206 - val_loss: 0.1610 - val_mae: 0.2831 - val_mse: 0.1610\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "27/27 - 4s - 154ms/step - loss: 0.1791 - mae: 0.3065 - mse: 0.1791 - val_loss: 0.0967 - val_mae: 0.2348 - val_mse: 0.0967\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "40/40 - 4s - 111ms/step - loss: 0.1593 - mae: 0.2826 - mse: 0.1593 - val_loss: 0.1103 - val_mae: 0.2477 - val_mse: 0.1103\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m -48900us/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.5s\n",
      "14/14 - 3s - 202ms/step - loss: 0.2061 - mae: 0.3322 - mse: 0.2061 - val_loss: 0.1400 - val_mae: 0.2646 - val_mse: 0.1400\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 117ms/step - loss: 0.1772 - mae: 0.3089 - mse: 0.1772 - val_loss: 0.1105 - val_mae: 0.2399 - val_mse: 0.1105\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "40/40 - 3s - 85ms/step - loss: 0.1519 - mae: 0.2875 - mse: 0.1519 - val_loss: 0.0902 - val_mae: 0.2204 - val_mse: 0.0902\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 4s - 250ms/step - loss: 0.1467 - mae: 0.2745 - mse: 0.1467 - val_loss: 0.0681 - val_mae: 0.1844 - val_mse: 0.0681\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.6s\n",
      "27/27 - 4s - 142ms/step - loss: 0.1064 - mae: 0.2394 - mse: 0.1064 - val_loss: 0.0510 - val_mae: 0.1676 - val_mse: 0.0510\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "40/40 - 4s - 88ms/step - loss: 0.0807 - mae: 0.2124 - mse: 0.0807 - val_loss: 0.0600 - val_mae: 0.1842 - val_mse: 0.0600\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "14/14 - 3s - 210ms/step - loss: 0.1845 - mae: 0.3132 - mse: 0.1845 - val_loss: 0.1109 - val_mae: 0.2251 - val_mse: 0.1109\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 1s - 31ms/step - loss: 0.1408 - mae: 0.2695 - mse: 0.1408 - val_loss: 0.0573 - val_mae: 0.1791 - val_mse: 0.0573\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   1.6s\n",
      "40/40 - 3s - 79ms/step - loss: 0.1083 - mae: 0.2418 - mse: 0.1083 - val_loss: 0.0692 - val_mae: 0.1985 - val_mse: 0.0692\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "14/14 - 3s - 210ms/step - loss: 0.2057 - mae: 0.3305 - mse: 0.2057 - val_loss: 0.1285 - val_mae: 0.2500 - val_mse: 0.1285\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 3s - 129ms/step - loss: 0.1798 - mae: 0.3093 - mse: 0.1798 - val_loss: 0.0988 - val_mae: 0.2267 - val_mse: 0.0988\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "40/40 - 3s - 83ms/step - loss: 0.1437 - mae: 0.2781 - mse: 0.1437 - val_loss: 0.0907 - val_mae: 0.2237 - val_mse: 0.0907\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "14/14 - 5s - 354ms/step - loss: 0.1630 - mae: 0.2891 - mse: 0.1630 - val_loss: 0.0663 - val_mae: 0.1829 - val_mse: 0.0663\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "27/27 - 5s - 177ms/step - loss: 0.1133 - mae: 0.2493 - mse: 0.1133 - val_loss: 0.0496 - val_mae: 0.1735 - val_mse: 0.0496\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "40/40 - 3s - 64ms/step - loss: 0.1051 - mae: 0.2378 - mse: 0.1051 - val_loss: 0.0637 - val_mae: 0.1902 - val_mse: 0.0637\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "14/14 - 3s - 202ms/step - loss: 0.1676 - mae: 0.3144 - mse: 0.1676 - val_loss: 0.0826 - val_mae: 0.2140 - val_mse: 0.0826\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 6s - 222ms/step - loss: 0.1208 - mae: 0.2564 - mse: 0.1208 - val_loss: 0.0528 - val_mae: 0.1741 - val_mse: 0.0528\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.9s\n",
      "40/40 - 4s - 94ms/step - loss: 0.1091 - mae: 0.2440 - mse: 0.1091 - val_loss: 0.0628 - val_mae: 0.1893 - val_mse: 0.0628\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "14/14 - 4s - 271ms/step - loss: 0.1926 - mae: 0.3149 - mse: 0.1926 - val_loss: 0.1129 - val_mae: 0.2274 - val_mse: 0.1129\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "27/27 - 3s - 127ms/step - loss: 0.1282 - mae: 0.2602 - mse: 0.1282 - val_loss: 0.0559 - val_mae: 0.1750 - val_mse: 0.0559\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "40/40 - 1s - 36ms/step - loss: 0.1054 - mae: 0.2345 - mse: 0.1054 - val_loss: 0.0652 - val_mae: 0.1950 - val_mse: 0.0652\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   2.3s\n",
      "14/14 - 3s - 218ms/step - loss: 0.1300 - mae: 0.2610 - mse: 0.1300 - val_loss: 0.0637 - val_mae: 0.1888 - val_mse: 0.0637\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "27/27 - 4s - 131ms/step - loss: 0.0970 - mae: 0.2310 - mse: 0.0970 - val_loss: 0.0475 - val_mae: 0.1631 - val_mse: 0.0475\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "40/40 - 4s - 89ms/step - loss: 0.0840 - mae: 0.2195 - mse: 0.0840 - val_loss: 0.0588 - val_mae: 0.1800 - val_mse: 0.0588\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "14/14 - 3s - 216ms/step - loss: 0.2074 - mae: 0.3279 - mse: 0.2074 - val_loss: 0.1471 - val_mae: 0.2657 - val_mse: 0.1471\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "27/27 - 4s - 130ms/step - loss: 0.1545 - mae: 0.2901 - mse: 0.1545 - val_loss: 0.0735 - val_mae: 0.2002 - val_mse: 0.0735\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "40/40 - 4s - 93ms/step - loss: 0.1675 - mae: 0.2867 - mse: 0.1675 - val_loss: 0.1184 - val_mae: 0.2428 - val_mse: 0.1184\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "14/14 - 3s - 185ms/step - loss: 0.2203 - mae: 0.3383 - mse: 0.2203 - val_loss: 0.1641 - val_mae: 0.2747 - val_mse: 0.1641\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "27/27 - 4s - 155ms/step - loss: 0.1766 - mae: 0.3084 - mse: 0.1766 - val_loss: 0.0926 - val_mae: 0.2234 - val_mse: 0.0926\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "40/40 - 5s - 117ms/step - loss: 0.1481 - mae: 0.2777 - mse: 0.1481 - val_loss: 0.0848 - val_mae: 0.2151 - val_mse: 0.0848\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "14/14 - 3s - 216ms/step - loss: 0.1974 - mae: 0.3243 - mse: 0.1974 - val_loss: 0.1221 - val_mae: 0.2317 - val_mse: 0.1221\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "27/27 - 8s - 286ms/step - loss: 0.1582 - mae: 0.2933 - mse: 0.1582 - val_loss: 0.0913 - val_mae: 0.2192 - val_mse: 0.0913\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.9s\n",
      "40/40 - 4s - 106ms/step - loss: 0.1464 - mae: 0.2804 - mse: 0.1464 - val_loss: 0.0885 - val_mae: 0.2186 - val_mse: 0.0885\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "14/14 - 4s - 254ms/step - loss: 0.2192 - mae: 0.3368 - mse: 0.2192 - val_loss: 0.1544 - val_mae: 0.2735 - val_mse: 0.1544\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "27/27 - 6s - 230ms/step - loss: 0.1971 - mae: 0.3191 - mse: 0.1971 - val_loss: 0.1208 - val_mae: 0.2497 - val_mse: 0.1208\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.7s\n",
      "40/40 - 5s - 131ms/step - loss: 0.1630 - mae: 0.2929 - mse: 0.1630 - val_loss: 0.1010 - val_mae: 0.2400 - val_mse: 0.1010\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.4s\n",
      "14/14 - 4s - 315ms/step - loss: 0.1728 - mae: 0.2940 - mse: 0.1728 - val_loss: 0.0784 - val_mae: 0.1913 - val_mse: 0.0784\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 5s - 169ms/step - loss: 0.1252 - mae: 0.2616 - mse: 0.1252 - val_loss: 0.0500 - val_mae: 0.1635 - val_mse: 0.0500\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "40/40 - 3s - 68ms/step - loss: 0.1091 - mae: 0.2404 - mse: 0.1091 - val_loss: 0.0656 - val_mae: 0.1924 - val_mse: 0.0656\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "14/14 - 3s - 238ms/step - loss: 0.1422 - mae: 0.2776 - mse: 0.1422 - val_loss: 0.0743 - val_mae: 0.2103 - val_mse: 0.0743\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 148ms/step - loss: 0.1114 - mae: 0.2503 - mse: 0.1114 - val_loss: 0.0488 - val_mae: 0.1688 - val_mse: 0.0488\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "40/40 - 4s - 107ms/step - loss: 0.0964 - mae: 0.2337 - mse: 0.0964 - val_loss: 0.0590 - val_mae: 0.1824 - val_mse: 0.0590\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "14/14 - 3s - 245ms/step - loss: 0.1402 - mae: 0.2736 - mse: 0.1402 - val_loss: 0.0728 - val_mae: 0.2031 - val_mse: 0.0728\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 4s - 141ms/step - loss: 0.1007 - mae: 0.2408 - mse: 0.1007 - val_loss: 0.0505 - val_mae: 0.1761 - val_mse: 0.0505\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "40/40 - 2s - 46ms/step - loss: 0.0849 - mae: 0.2221 - mse: 0.0849 - val_loss: 0.0600 - val_mae: 0.1822 - val_mse: 0.0600\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   2.9s\n",
      "14/14 - 4s - 305ms/step - loss: 0.2075 - mae: 0.3303 - mse: 0.2075 - val_loss: 0.1360 - val_mae: 0.2502 - val_mse: 0.1360\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 4s - 162ms/step - loss: 0.1622 - mae: 0.2924 - mse: 0.1622 - val_loss: 0.0683 - val_mae: 0.1948 - val_mse: 0.0683\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "40/40 - 9s - 220ms/step - loss: 0.1302 - mae: 0.2665 - mse: 0.1302 - val_loss: 0.0742 - val_mae: 0.2102 - val_mse: 0.0742\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  11.0s\n",
      "14/14 - 5s - 335ms/step - loss: 0.1900 - mae: 0.3130 - mse: 0.1900 - val_loss: 0.1202 - val_mae: 0.2469 - val_mse: 0.1202\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 2s - 70ms/step - loss: 0.1513 - mae: 0.2856 - mse: 0.1513 - val_loss: 0.0763 - val_mae: 0.2028 - val_mse: 0.0763\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   2.9s\n",
      "40/40 - 4s - 105ms/step - loss: 0.1457 - mae: 0.2789 - mse: 0.1457 - val_loss: 0.0909 - val_mae: 0.2287 - val_mse: 0.0909\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=50, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "53/53 - 4s - 78ms/step - loss: 0.0854 - mae: 0.2208 - mse: 0.0854 - val_loss: 0.0768 - val_mae: 0.2157 - val_mse: 0.0768\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [50]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params_50 = []\n",
    "best_score_50 = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[numerical].drop(columns=['Time']))\n",
    "\n",
    "    # preprocess data (2D)\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 60) \n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 60) \n",
    "\n",
    "    # print(f'shape of y: {Y.shape}')\n",
    "    for conv in conv_layer: \n",
    "        # will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\n",
    "        \n",
    "        if conv: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [60],\n",
    "        'n_predicted_features' : [7],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"mse\"],\n",
    "        'metrics': [[\"mse\", 'mae']],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], # 32, 64\n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [False],\n",
    "        'epochs':[10, 50], # 50\n",
    "        'batch':[64],\n",
    "        }\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter=20,  # Number of different combinations to sample\n",
    "            scoring = make_scorer(custom_mse), \n",
    "            cv=tscv,  # Number of cross-validation folds\n",
    "            verbose=2,  # Display search progress\n",
    "            n_jobs=1,  # Use all available CPUs\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params_50.append(random_search.best_params_)\n",
    "        best_score_50.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 50 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 50, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
      "Best Scorefor 50 npast_timesteps: -0.07934705068987576\n"
     ]
    }
   ],
   "source": [
    "# take the best score and find associated parameters\n",
    "\n",
    "max_index_50 = best_score_50.index(max(best_score_50))\n",
    "print(f'Best Params for 50 npast_timesteps: {best_params_50[max_index_50]}')\n",
    "print(f'Best Scorefor 50 npast_timesteps: {max(best_score_50)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Params for 50 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 50, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
    "Best Scorefor 50 npast_timesteps: -0.07934705068987576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_scores_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbest_scores_\u001b[49m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmax\u001b[39m(best_score_50))\n\u001b[1;32m      2\u001b[0m best_params_\u001b[38;5;241m.\u001b[39mappend(best_params_50[max_index_50])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_scores_' is not defined"
     ]
    }
   ],
   "source": [
    "best_scores_.append(max(best_score_50))\n",
    "best_params_.append(best_params_50[max_index_50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75 npast_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:04:58.003187: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 3s - 234ms/step - loss: 0.2191 - mae: 0.3460 - mse: 0.2191 - val_loss: 0.1570 - val_mae: 0.2761 - val_mse: 0.1570\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "27/27 - 4s - 151ms/step - loss: 0.1373 - mae: 0.2679 - mse: 0.1373 - val_loss: 0.0620 - val_mae: 0.1808 - val_mse: 0.0620\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.1s\n",
      "40/40 - 5s - 126ms/step - loss: 0.1284 - mae: 0.2531 - mse: 0.1284 - val_loss: 0.0840 - val_mae: 0.2100 - val_mse: 0.0840\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "14/14 - 3s - 222ms/step - loss: 0.1701 - mae: 0.3126 - mse: 0.1701 - val_loss: 0.0786 - val_mae: 0.2137 - val_mse: 0.0786\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 4s - 143ms/step - loss: 0.1255 - mae: 0.2684 - mse: 0.1255 - val_loss: 0.0614 - val_mae: 0.1854 - val_mse: 0.0614\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "40/40 - 3s - 70ms/step - loss: 0.1056 - mae: 0.2349 - mse: 0.1056 - val_loss: 0.0709 - val_mae: 0.2000 - val_mse: 0.0709\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "14/14 - 7s - 492ms/step - loss: 0.1968 - mae: 0.3304 - mse: 0.1968 - val_loss: 0.1135 - val_mae: 0.2382 - val_mse: 0.1135\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "27/27 - 8s - 281ms/step - loss: 0.1468 - mae: 0.2801 - mse: 0.1468 - val_loss: 0.0738 - val_mae: 0.1960 - val_mse: 0.0738\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.4s\n",
      "40/40 - 8s - 202ms/step - loss: 0.1223 - mae: 0.2536 - mse: 0.1223 - val_loss: 0.0673 - val_mae: 0.1966 - val_mse: 0.0673\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.9s\n",
      "14/14 - 3s - 226ms/step - loss: 0.2234 - mae: 0.3488 - mse: 0.2234 - val_loss: 0.1613 - val_mae: 0.2904 - val_mse: 0.1613\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 155ms/step - loss: 0.1303 - mae: 0.2692 - mse: 0.1303 - val_loss: 0.0804 - val_mae: 0.2113 - val_mse: 0.0804\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "40/40 - 5s - 137ms/step - loss: 0.1393 - mae: 0.2792 - mse: 0.1393 - val_loss: 0.0826 - val_mae: 0.2138 - val_mse: 0.0826\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.7s\n",
      "14/14 - 4s - 312ms/step - loss: 0.1304 - mae: 0.2653 - mse: 0.1304 - val_loss: 0.0686 - val_mae: 0.1938 - val_mse: 0.0686\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 5s - 171ms/step - loss: 0.0986 - mae: 0.2385 - mse: 0.0986 - val_loss: 0.0569 - val_mae: 0.1844 - val_mse: 0.0569\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "40/40 - 4s - 111ms/step - loss: 0.0894 - mae: 0.2214 - mse: 0.0894 - val_loss: 0.0648 - val_mae: 0.1893 - val_mse: 0.0648\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "14/14 - 4s - 264ms/step - loss: 0.1861 - mae: 0.3187 - mse: 0.1861 - val_loss: 0.0924 - val_mae: 0.2084 - val_mse: 0.0924\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "27/27 - 5s - 186ms/step - loss: 0.1022 - mae: 0.2479 - mse: 0.1022 - val_loss: 0.0531 - val_mae: 0.1704 - val_mse: 0.0531\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.2s\n",
      "40/40 - 6s - 140ms/step - loss: 0.1162 - mae: 0.2527 - mse: 0.1162 - val_loss: 0.0607 - val_mae: 0.1847 - val_mse: 0.0607\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "14/14 - 4s - 265ms/step - loss: 0.1986 - mae: 0.3351 - mse: 0.1986 - val_loss: 0.1298 - val_mae: 0.2557 - val_mse: 0.1298\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 2s - 83ms/step - loss: 0.1633 - mae: 0.2965 - mse: 0.1633 - val_loss: 0.0939 - val_mae: 0.2247 - val_mse: 0.0939\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.3s\n",
      "40/40 - 5s - 129ms/step - loss: 0.1200 - mae: 0.2556 - mse: 0.1200 - val_loss: 0.0820 - val_mae: 0.2205 - val_mse: 0.0820\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "14/14 - 6s - 431ms/step - loss: 0.1262 - mae: 0.2671 - mse: 0.1262 - val_loss: 0.0785 - val_mae: 0.1977 - val_mse: 0.0785\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.1s\n",
      "27/27 - 10s - 360ms/step - loss: 0.0998 - mae: 0.2367 - mse: 0.0998 - val_loss: 0.0493 - val_mae: 0.1684 - val_mse: 0.0493\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.9s\n",
      "40/40 - 10s - 254ms/step - loss: 0.0870 - mae: 0.2248 - mse: 0.0870 - val_loss: 0.0613 - val_mae: 0.1913 - val_mse: 0.0613\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.6s\n",
      "14/14 - 4s - 308ms/step - loss: 0.1943 - mae: 0.3303 - mse: 0.1943 - val_loss: 0.1050 - val_mae: 0.2240 - val_mse: 0.1050\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.7s\n",
      "27/27 - 5s - 201ms/step - loss: 0.1205 - mae: 0.2602 - mse: 0.1205 - val_loss: 0.0602 - val_mae: 0.1794 - val_mse: 0.0602\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.6s\n",
      "40/40 - 6s - 143ms/step - loss: 0.1021 - mae: 0.2356 - mse: 0.1021 - val_loss: 0.0669 - val_mae: 0.1962 - val_mse: 0.0669\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.0s\n",
      "14/14 - 2s - 165ms/step - loss: 0.1991 - mae: 0.3367 - mse: 0.1991 - val_loss: 0.1166 - val_mae: 0.2464 - val_mse: 0.1166\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 6s - 219ms/step - loss: 0.0985 - mae: 0.2421 - mse: 0.0985 - val_loss: 0.0577 - val_mae: 0.1812 - val_mse: 0.0577\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "40/40 - 8s - 207ms/step - loss: 0.0901 - mae: 0.2241 - mse: 0.0901 - val_loss: 0.0682 - val_mae: 0.1911 - val_mse: 0.0682\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.7s\n",
      "14/14 - 4s - 299ms/step - loss: 0.1690 - mae: 0.3011 - mse: 0.1690 - val_loss: 0.0893 - val_mae: 0.2224 - val_mse: 0.0893\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 3s - 111ms/step - loss: 0.1112 - mae: 0.2496 - mse: 0.1112 - val_loss: 0.0579 - val_mae: 0.1846 - val_mse: 0.0579\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "40/40 - 7s - 185ms/step - loss: 0.0990 - mae: 0.2330 - mse: 0.0990 - val_loss: 0.0694 - val_mae: 0.1987 - val_mse: 0.0694\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "14/14 - 4s - 273ms/step - loss: 0.2227 - mae: 0.3481 - mse: 0.2227 - val_loss: 0.1548 - val_mae: 0.2748 - val_mse: 0.1548\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "27/27 - 5s - 187ms/step - loss: 0.1757 - mae: 0.3061 - mse: 0.1757 - val_loss: 0.1010 - val_mae: 0.2403 - val_mse: 0.1010\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "40/40 - 7s - 182ms/step - loss: 0.1113 - mae: 0.2414 - mse: 0.1113 - val_loss: 0.0923 - val_mae: 0.2271 - val_mse: 0.0923\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m 25ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.4s\n",
      "14/14 - 7s - 531ms/step - loss: 0.1996 - mae: 0.3272 - mse: 0.1996 - val_loss: 0.1248 - val_mae: 0.2482 - val_mse: 0.1248\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.1s\n",
      "27/27 - 10s - 367ms/step - loss: 0.1262 - mae: 0.2611 - mse: 0.1262 - val_loss: 0.0550 - val_mae: 0.1685 - val_mse: 0.0550\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  12.0s\n",
      "40/40 - 12s - 294ms/step - loss: 0.1465 - mae: 0.2787 - mse: 0.1465 - val_loss: 0.0670 - val_mae: 0.1902 - val_mse: 0.0670\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  14.3s\n",
      "14/14 - 5s - 326ms/step - loss: 0.1784 - mae: 0.3083 - mse: 0.1784 - val_loss: 0.1098 - val_mae: 0.2295 - val_mse: 0.1098\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.0s\n",
      "27/27 - 6s - 239ms/step - loss: 0.1542 - mae: 0.2888 - mse: 0.1542 - val_loss: 0.0816 - val_mae: 0.2044 - val_mse: 0.0816\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.1s\n",
      "40/40 - 7s - 183ms/step - loss: 0.1336 - mae: 0.2591 - mse: 0.1336 - val_loss: 0.0898 - val_mae: 0.2203 - val_mse: 0.0898\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.6s\n",
      "14/14 - 6s - 428ms/step - loss: 0.1934 - mae: 0.3276 - mse: 0.1934 - val_loss: 0.1294 - val_mae: 0.2535 - val_mse: 0.1294\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   9.4s\n",
      "27/27 - 12s - 443ms/step - loss: 0.1393 - mae: 0.2757 - mse: 0.1393 - val_loss: 0.0564 - val_mae: 0.1775 - val_mse: 0.0564\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  14.6s\n",
      "40/40 - 11s - 273ms/step - loss: 0.1013 - mae: 0.2362 - mse: 0.1013 - val_loss: 0.0667 - val_mae: 0.1901 - val_mse: 0.0667\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  13.5s\n",
      "14/14 - 7s - 488ms/step - loss: 0.1331 - mae: 0.2743 - mse: 0.1331 - val_loss: 0.0771 - val_mae: 0.2028 - val_mse: 0.0771\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  10.1s\n",
      "27/27 - 11s - 398ms/step - loss: 0.1008 - mae: 0.2389 - mse: 0.1008 - val_loss: 0.0490 - val_mae: 0.1730 - val_mse: 0.0490\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  13.6s\n",
      "40/40 - 12s - 311ms/step - loss: 0.0946 - mae: 0.2333 - mse: 0.0946 - val_loss: 0.0597 - val_mae: 0.1841 - val_mse: 0.0597\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=  15.1s\n",
      "14/14 - 5s - 328ms/step - loss: 0.1619 - mae: 0.3057 - mse: 0.1619 - val_loss: 0.0884 - val_mae: 0.2114 - val_mse: 0.0884\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "27/27 - 6s - 221ms/step - loss: 0.1204 - mae: 0.2660 - mse: 0.1204 - val_loss: 0.0602 - val_mae: 0.1824 - val_mse: 0.0602\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "40/40 - 8s - 206ms/step - loss: 0.0981 - mae: 0.2367 - mse: 0.0981 - val_loss: 0.0619 - val_mae: 0.1850 - val_mse: 0.0619\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.0s\n",
      "14/14 - 4s - 310ms/step - loss: 0.1358 - mae: 0.2756 - mse: 0.1358 - val_loss: 0.0746 - val_mae: 0.2129 - val_mse: 0.0746\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.9s\n",
      "27/27 - 7s - 248ms/step - loss: 0.1084 - mae: 0.2482 - mse: 0.1084 - val_loss: 0.0547 - val_mae: 0.1718 - val_mse: 0.0547\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   8.8s\n",
      "40/40 - 6s - 157ms/step - loss: 0.0932 - mae: 0.2280 - mse: 0.0932 - val_loss: 0.0613 - val_mae: 0.1818 - val_mse: 0.0613\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  13.7s\n",
      "14/14 - 8s - 553ms/step - loss: 0.1787 - mae: 0.3198 - mse: 0.1787 - val_loss: 0.1146 - val_mae: 0.2334 - val_mse: 0.1146\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  10.7s\n",
      "27/27 - 11s - 402ms/step - loss: 0.1341 - mae: 0.2705 - mse: 0.1341 - val_loss: 0.0533 - val_mae: 0.1758 - val_mse: 0.0533\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  12.0s\n",
      "40/40 - 14s - 348ms/step - loss: 0.1293 - mae: 0.2632 - mse: 0.1293 - val_loss: 0.0714 - val_mae: 0.2017 - val_mse: 0.0714\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=False, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  16.9s\n",
      "14/14 - 4s - 277ms/step - loss: 0.2155 - mae: 0.3404 - mse: 0.2155 - val_loss: 0.1621 - val_mae: 0.2771 - val_mse: 0.1621\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "27/27 - 5s - 203ms/step - loss: 0.1393 - mae: 0.2651 - mse: 0.1393 - val_loss: 0.0661 - val_mae: 0.1886 - val_mse: 0.0661\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.8s\n",
      "40/40 - 5s - 125ms/step - loss: 0.1380 - mae: 0.2695 - mse: 0.1380 - val_loss: 0.1017 - val_mae: 0.2400 - val_mse: 0.1017\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=False, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxlee/miniconda3/envs/msse-python/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 10s - 183ms/step - loss: 0.1016 - mae: 0.2385 - mse: 0.1016 - val_loss: 0.0780 - val_mae: 0.2211 - val_mse: 0.0780\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "14/14 - 5s - 356ms/step - loss: 0.1912 - mae: 0.3344 - mse: 0.1912 - val_loss: 0.1206 - val_mae: 0.2562 - val_mse: 0.1206\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.2s\n",
      "27/27 - 4s - 155ms/step - loss: 0.1490 - mae: 0.2838 - mse: 0.1490 - val_loss: 0.0748 - val_mae: 0.1974 - val_mse: 0.0748\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "40/40 - 2s - 38ms/step - loss: 0.1279 - mae: 0.2657 - mse: 0.1279 - val_loss: 0.0649 - val_mae: 0.1914 - val_mse: 0.0649\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   2.4s\n",
      "14/14 - 3s - 224ms/step - loss: 0.1595 - mae: 0.2941 - mse: 0.1595 - val_loss: 0.0827 - val_mae: 0.2220 - val_mse: 0.0827\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "27/27 - 3s - 123ms/step - loss: 0.1063 - mae: 0.2412 - mse: 0.1063 - val_loss: 0.0495 - val_mae: 0.1630 - val_mse: 0.0495\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "40/40 - 4s - 98ms/step - loss: 0.0869 - mae: 0.2184 - mse: 0.0869 - val_loss: 0.0623 - val_mae: 0.1891 - val_mse: 0.0623\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.0s\n",
      "14/14 - 4s - 312ms/step - loss: 0.2086 - mae: 0.3393 - mse: 0.2086 - val_loss: 0.1475 - val_mae: 0.2953 - val_mse: 0.1475\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "27/27 - 5s - 184ms/step - loss: 0.1643 - mae: 0.2956 - mse: 0.1643 - val_loss: 0.0736 - val_mae: 0.1949 - val_mse: 0.0736\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.3s\n",
      "40/40 - 3s - 66ms/step - loss: 0.1432 - mae: 0.2695 - mse: 0.1432 - val_loss: 0.0832 - val_mae: 0.2217 - val_mse: 0.0832\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.7s\n",
      "14/14 - 3s - 223ms/step - loss: 0.1902 - mae: 0.3207 - mse: 0.1902 - val_loss: 0.1259 - val_mae: 0.2429 - val_mse: 0.1259\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "27/27 - 4s - 139ms/step - loss: 0.1518 - mae: 0.2922 - mse: 0.1518 - val_loss: 0.0663 - val_mae: 0.1818 - val_mse: 0.0663\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.8s\n",
      "40/40 - 7s - 164ms/step - loss: 0.1395 - mae: 0.2752 - mse: 0.1395 - val_loss: 0.0754 - val_mae: 0.2050 - val_mse: 0.0754\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.6s\n",
      "14/14 - 3s - 242ms/step - loss: 0.1188 - mae: 0.2501 - mse: 0.1188 - val_loss: 0.0583 - val_mae: 0.1823 - val_mse: 0.0583\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "27/27 - 4s - 141ms/step - loss: 0.0943 - mae: 0.2300 - mse: 0.0943 - val_loss: 0.0460 - val_mae: 0.1691 - val_mse: 0.0460\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.2s\n",
      "40/40 - 4s - 97ms/step - loss: 0.0876 - mae: 0.2232 - mse: 0.0876 - val_loss: 0.0570 - val_mae: 0.1765 - val_mse: 0.0570\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.9s\n",
      "14/14 - 3s - 225ms/step - loss: 0.1765 - mae: 0.3020 - mse: 0.1765 - val_loss: 0.0911 - val_mae: 0.2055 - val_mse: 0.0911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   2.2s\n",
      "27/27 - 3s - 120ms/step - loss: 0.1324 - mae: 0.2658 - mse: 0.1324 - val_loss: 0.0521 - val_mae: 0.1782 - val_mse: 0.0521\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 3s - 84ms/step - loss: 0.1027 - mae: 0.2335 - mse: 0.1027 - val_loss: 0.0615 - val_mae: 0.1846 - val_mse: 0.0615\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "14/14 - 3s - 234ms/step - loss: 0.1841 - mae: 0.3187 - mse: 0.1841 - val_loss: 0.1171 - val_mae: 0.2280 - val_mse: 0.1171\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "27/27 - 3s - 123ms/step - loss: 0.1362 - mae: 0.2747 - mse: 0.1362 - val_loss: 0.0514 - val_mae: 0.1673 - val_mse: 0.0514\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "40/40 - 3s - 84ms/step - loss: 0.1312 - mae: 0.2697 - mse: 0.1312 - val_loss: 0.0699 - val_mae: 0.1965 - val_mse: 0.0699\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "14/14 - 5s - 323ms/step - loss: 0.1629 - mae: 0.2974 - mse: 0.1629 - val_loss: 0.0736 - val_mae: 0.2084 - val_mse: 0.0736\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "27/27 - 3s - 109ms/step - loss: 0.1160 - mae: 0.2485 - mse: 0.1160 - val_loss: 0.0474 - val_mae: 0.1622 - val_mse: 0.0474\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 5s - 130ms/step - loss: 0.0984 - mae: 0.2292 - mse: 0.0984 - val_loss: 0.0632 - val_mae: 0.1879 - val_mse: 0.0632\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.4s\n",
      "14/14 - 7s - 485ms/step - loss: 0.1533 - mae: 0.2887 - mse: 0.1533 - val_loss: 0.0718 - val_mae: 0.1924 - val_mse: 0.0718\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   7.8s\n",
      "27/27 - 4s - 130ms/step - loss: 0.1083 - mae: 0.2439 - mse: 0.1083 - val_loss: 0.0483 - val_mae: 0.1624 - val_mse: 0.0483\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "40/40 - 4s - 102ms/step - loss: 0.1027 - mae: 0.2361 - mse: 0.1027 - val_loss: 0.0650 - val_mae: 0.1932 - val_mse: 0.0650\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.3s\n",
      "14/14 - 1s - 103ms/step - loss: 0.1753 - mae: 0.2982 - mse: 0.1753 - val_loss: 0.0847 - val_mae: 0.1962 - val_mse: 0.0847\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   2.3s\n",
      "27/27 - 3s - 120ms/step - loss: 0.1413 - mae: 0.2709 - mse: 0.1413 - val_loss: 0.0591 - val_mae: 0.1903 - val_mse: 0.0591\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.1s\n",
      "40/40 - 3s - 85ms/step - loss: 0.1009 - mae: 0.2334 - mse: 0.1009 - val_loss: 0.0636 - val_mae: 0.1872 - val_mse: 0.0636\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.3s\n",
      "14/14 - 3s - 213ms/step - loss: 0.1053 - mae: 0.2411 - mse: 0.1053 - val_loss: 0.0620 - val_mae: 0.2019 - val_mse: 0.0620\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 116ms/step - loss: 0.0893 - mae: 0.2241 - mse: 0.0893 - val_loss: 0.0421 - val_mae: 0.1603 - val_mse: 0.0421\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "40/40 - 4s - 89ms/step - loss: 0.0790 - mae: 0.2132 - mse: 0.0790 - val_loss: 0.0510 - val_mae: 0.1677 - val_mse: 0.0510\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.5s\n",
      "14/14 - 3s - 204ms/step - loss: 0.2048 - mae: 0.3281 - mse: 0.2048 - val_loss: 0.1253 - val_mae: 0.2591 - val_mse: 0.1253\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.6s\n",
      "27/27 - 3s - 119ms/step - loss: 0.1413 - mae: 0.2766 - mse: 0.1413 - val_loss: 0.0732 - val_mae: 0.1915 - val_mse: 0.0732\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "40/40 - 1s - 28ms/step - loss: 0.1201 - mae: 0.2556 - mse: 0.1201 - val_loss: 0.0676 - val_mae: 0.1953 - val_mse: 0.0676\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   2.1s\n",
      "14/14 - 4s - 302ms/step - loss: 0.2020 - mae: 0.3190 - mse: 0.2020 - val_loss: 0.1227 - val_mae: 0.2382 - val_mse: 0.1227\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.4s\n",
      "27/27 - 4s - 160ms/step - loss: 0.1772 - mae: 0.3020 - mse: 0.1772 - val_loss: 0.0890 - val_mae: 0.2085 - val_mse: 0.0890\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "40/40 - 4s - 110ms/step - loss: 0.1244 - mae: 0.2615 - mse: 0.1244 - val_loss: 0.0611 - val_mae: 0.1826 - val_mse: 0.0611\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.2, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "14/14 - 3s - 220ms/step - loss: 0.1807 - mae: 0.3165 - mse: 0.1807 - val_loss: 0.1250 - val_mae: 0.2378 - val_mse: 0.1250\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   7.3s\n",
      "27/27 - 5s - 172ms/step - loss: 0.1609 - mae: 0.3005 - mse: 0.1609 - val_loss: 0.0753 - val_mae: 0.2086 - val_mse: 0.0753\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "40/40 - 2s - 47ms/step - loss: 0.1644 - mae: 0.2968 - mse: 0.1644 - val_loss: 0.1018 - val_mae: 0.2341 - val_mse: 0.1018\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.1s\n",
      "14/14 - 5s - 343ms/step - loss: 0.2018 - mae: 0.3263 - mse: 0.2018 - val_loss: 0.1221 - val_mae: 0.2488 - val_mse: 0.1221\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "27/27 - 5s - 171ms/step - loss: 0.1730 - mae: 0.2991 - mse: 0.1730 - val_loss: 0.1023 - val_mae: 0.2294 - val_mse: 0.1023\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.8s\n",
      "40/40 - 5s - 129ms/step - loss: 0.1431 - mae: 0.2720 - mse: 0.1431 - val_loss: 0.0786 - val_mae: 0.2150 - val_mse: 0.0786\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.4s\n",
      "14/14 - 5s - 347ms/step - loss: 0.1648 - mae: 0.2889 - mse: 0.1648 - val_loss: 0.0810 - val_mae: 0.2030 - val_mse: 0.0810\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   6.1s\n",
      "27/27 - 3s - 102ms/step - loss: 0.1141 - mae: 0.2429 - mse: 0.1141 - val_loss: 0.0486 - val_mae: 0.1689 - val_mse: 0.0486\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   3.9s\n",
      "40/40 - 4s - 112ms/step - loss: 0.0985 - mae: 0.2294 - mse: 0.0985 - val_loss: 0.0661 - val_mae: 0.1999 - val_mse: 0.0661\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=32, optimizer=adam, pool_size=2; total time=   5.6s\n",
      "14/14 - 3s - 203ms/step - loss: 0.1479 - mae: 0.2858 - mse: 0.1479 - val_loss: 0.0681 - val_mae: 0.1994 - val_mse: 0.0681\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 127ms/step - loss: 0.1109 - mae: 0.2513 - mse: 0.1109 - val_loss: 0.0459 - val_mae: 0.1681 - val_mse: 0.0459\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "40/40 - 3s - 87ms/step - loss: 0.0904 - mae: 0.2247 - mse: 0.0904 - val_loss: 0.0587 - val_mae: 0.1812 - val_mse: 0.0587\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.3, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.4s\n",
      "14/14 - 3s - 211ms/step - loss: 0.1202 - mae: 0.2532 - mse: 0.1202 - val_loss: 0.0666 - val_mae: 0.2000 - val_mse: 0.0666\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   3.8s\n",
      "27/27 - 3s - 117ms/step - loss: 0.0871 - mae: 0.2225 - mse: 0.0871 - val_loss: 0.0459 - val_mae: 0.1617 - val_mse: 0.0459\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.0s\n",
      "40/40 - 3s - 85ms/step - loss: 0.0876 - mae: 0.2286 - mse: 0.0876 - val_loss: 0.0596 - val_mae: 0.1826 - val_mse: 0.0596\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-1s\u001b[0m -51957us/step\n",
      "[CV] END activation=tanh, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.3, epochs=50, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   2.2s\n",
      "14/14 - 4s - 313ms/step - loss: 0.1915 - mae: 0.3279 - mse: 0.1915 - val_loss: 0.1190 - val_mae: 0.2550 - val_mse: 0.1190\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   5.5s\n",
      "27/27 - 5s - 188ms/step - loss: 0.1506 - mae: 0.2836 - mse: 0.1506 - val_loss: 0.0626 - val_mae: 0.1773 - val_mse: 0.0626\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   6.5s\n",
      "40/40 - 10s - 245ms/step - loss: 0.1204 - mae: 0.2538 - mse: 0.1204 - val_loss: 0.0633 - val_mae: 0.1933 - val_mse: 0.0633\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=tanh, conv_layer=True, dropout=0.2, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[128, 64, 32], nfilters=64, optimizer=adam, pool_size=2; total time=  11.2s\n",
      "14/14 - 4s - 273ms/step - loss: 0.1930 - mae: 0.3197 - mse: 0.1930 - val_loss: 0.1007 - val_mae: 0.2257 - val_mse: 0.1007\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.7s\n",
      "27/27 - 3s - 123ms/step - loss: 0.1595 - mae: 0.2950 - mse: 0.1595 - val_loss: 0.0687 - val_mae: 0.1984 - val_mse: 0.0687\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   4.2s\n",
      "40/40 - 2s - 42ms/step - loss: 0.1163 - mae: 0.2476 - mse: 0.1163 - val_loss: 0.0801 - val_mae: 0.2048 - val_mse: 0.0801\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "[CV] END activation=relu, batch=64, classification=False, conv_act=relu, conv_layer=True, dropout=0.1, epochs=10, loss=mse, metrics=['mse', 'mae'], n_features=7, n_predicted_features=7, n_predicted_timesteps=60, n_timesteps=75, neurons=[32, 32], nfilters=64, optimizer=adam, pool_size=2; total time=   2.7s\n",
      "53/53 - 5s - 92ms/step - loss: 0.0804 - mae: 0.2169 - mse: 0.0804 - val_loss: 0.0798 - val_mae: 0.2213 - val_mse: 0.0798\n"
     ]
    }
   ],
   "source": [
    "past_timepoints = [75]\n",
    "conv_layer = [False, True]\n",
    "\n",
    "best_params_75 = []\n",
    "best_score_75 = []\n",
    "\n",
    "for i in past_timepoints:\n",
    "    # reset x and y\n",
    "    X = np.array(data[numerical].drop(columns=['Time']))\n",
    "    Y = np.array(data[numerical].drop(columns=['Time']))\n",
    "\n",
    "    # preprocess data (2D)\n",
    "    scalerX, X_norm = utils.scale(X)\n",
    "    scalerY, Y_norm = utils.scale(Y)\n",
    "    # print(f'x_shape scaled: {X_norm.shape}')\n",
    "    # print(f'y_shape scaled: {Y_norm.shape}')\n",
    "\n",
    "    # Reshape based on timesteps\n",
    "    [X, _] = utils.prep_lstm_data(X_norm, i, 60) \n",
    "    [_, Y] = utils.prep_lstm_data(Y_norm, i, 60) \n",
    "\n",
    "    # print(f'shape of y: {Y.shape}')\n",
    "    for conv in conv_layer: \n",
    "        # will execute with conv_layer False, then conv_layer True, \n",
    "        # must be in this order to prevent input shape error\n",
    "        # The data will reset to 2D when new npast_timesteps in the outter for loop\n",
    "        \n",
    "        if conv: # reshape input\n",
    "            X = X.reshape((X.shape[0], 1, X.shape[1], X.shape[2]))\n",
    "            \n",
    "        # define parameters to gridsearch\n",
    "        param_dict = {\n",
    "        'neurons': [[32, 32], [64, 32], [128, 64, 32]],\n",
    "        'activation':[\"tanh\", 'relu'],\n",
    "        'n_timesteps':[i],\n",
    "        'n_features':[7],\n",
    "        'n_predicted_timesteps': [60],\n",
    "        'n_predicted_features' : [7],\n",
    "        'optimizer': [\"adam\"],\n",
    "        'loss': [\"mse\"],\n",
    "        'metrics': [[\"mse\", 'mae']],\n",
    "        'dropout':[0.1, 0.2, 0.3],\n",
    "        'conv_layer':[conv],\n",
    "        'nfilters':[32, 64], # 32, 64\n",
    "        'conv_act': ['relu', 'tanh'],\n",
    "        'pool_size':[2],\n",
    "        'classification': [False],\n",
    "        'epochs':[10, 50], # 50\n",
    "        'batch':[64],\n",
    "        }\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "\n",
    "        # print(Y.shape)\n",
    "        # print(type(Y[0][0][0]))\n",
    "\n",
    "        # Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator= PreprocessingWrapper(),\n",
    "            param_distributions=param_dict,\n",
    "            n_iter=20,  # Number of different combinations to sample\n",
    "            scoring = make_scorer(custom_mse), \n",
    "            cv=tscv,  # Number of cross-validation folds\n",
    "            verbose=2,  # Display search progress\n",
    "            n_jobs= 1,  # Use all available CPUs\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X, Y)\n",
    "        best_params_75.append(random_search.best_params_)\n",
    "        best_score_75.append(random_search.best_score_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for 75 npast_timesteps: {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
      "Best Scorefor 75 npast_timesteps: -0.07810362410263826\n"
     ]
    }
   ],
   "source": [
    "# take the best score and find associated parameters\n",
    "\n",
    "max_index_75 = best_score_75.index(max(best_score_75))\n",
    "print(f'Best Params for 75 npast_timesteps: {best_params_75[max_index_75]}')\n",
    "print(f'Best Scorefor 75 npast_timesteps: {max(best_score_75)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add manually to list becuase kernal crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07934705068987576\n",
      "{'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "scores = [-0.07911169500806445, -0.07934705068987576, -0.07810362410263826]\n",
    "params = [{'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [128, 64, 32], 'n_timesteps': 25, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.3, 'conv_layer': False, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}, \n",
    "          {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 50, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'},\n",
    "            {'pool_size': 2, 'optimizer': 'adam', 'nfilters': 64, 'neurons': [64, 32], 'n_timesteps': 75, 'n_predicted_timesteps': 60, 'n_predicted_features': 7, 'n_features': 7, 'metrics': ['mse', 'mae'], 'loss': 'mse', 'epochs': 10, 'dropout': 0.2, 'conv_layer': True, 'conv_act': 'tanh', 'classification': False, 'batch': 64, 'activation': 'tanh'}\n",
    "]\n",
    "max_index = scores.index(max(scores))\n",
    "print(min(scores))\n",
    "print(params[max_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
